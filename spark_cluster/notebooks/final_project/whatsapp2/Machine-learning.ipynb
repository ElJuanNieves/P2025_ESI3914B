{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c898335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b655baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b064e8c8-9536-49c9-a832-718fe7ff253d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.4 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.4 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 807ms :: artifacts dl 29ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;3.5.4 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b064e8c8-9536-49c9-a832-718fe7ff253d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/18ms)\n",
      "25/05/11 06:22:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Proyecto\") \\\n",
    "    .master(\"spark://f04d2745dc57:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a0f30",
   "metadata": {},
   "source": [
    "### Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caeeba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 06:28:58 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "25/05/11 06:28:59 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+-------------------+-----+--------+------+\n",
      "|platform|    user_id|  post_id|         event_time|likes|comments|shares|\n",
      "+--------+-----------+---------+-------------------+-----+--------+------+\n",
      "|Facebook|user_fb_172|post_4198|2025-05-11 00:06:59| 1748|     695|    22|\n",
      "|Facebook|user_fb_438|post_3271|2025-05-10 23:51:30|  517|     631|    20|\n",
      "|Facebook|user_fb_371|post_6361|2025-05-10 23:51:32| 1084|     696|    39|\n",
      "|Facebook| user_fb_66|post_8041|2025-05-10 23:51:35| 2592|     191|    40|\n",
      "|Facebook|user_fb_302|post_2099|2025-05-10 23:43:21|  124|     498|   197|\n",
      "+--------+-----------+---------+-------------------+-----+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "#archivosSQL\n",
    "csvs = \"/home/jovyan/notebooks/final_project/whatsapp2/data/ml/ml_input\"\n",
    "\n",
    "#Almacenar los DataFrames de los CSVs que esten limpios\n",
    "csvs_clean = []\n",
    "\n",
    "# Itera sobre todos los archivos en el directorio\n",
    "for f in os.listdir(csvs):\n",
    "    if f.endswith(\".csv\"): \n",
    "        path = os.path.join(csvs, f)  #Obtener la ruta del archivo\n",
    "        try:\n",
    "            #Leer el CSV\n",
    "            df_temp = spark.read.csv(path, header=True, inferSchema=True)\n",
    "            _ = df_temp.count()  #Detectar errores de lectura\n",
    "            csvs_clean.append(df_temp) \n",
    "        except:\n",
    "            #Si tiene errores se ignora el archivo\n",
    "            print(f\"Archivo con errores: {f}\")\n",
    "\n",
    "#Si hay archivos limpios\n",
    "if csvs_clean:\n",
    "    #Combinar todos los DataFrames válidos en uno solo\n",
    "    df = reduce(DataFrame.unionAll, csvs_clean)\n",
    "    df.show(5)\n",
    "else:\n",
    "    print(\"Archivos inválidos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d20091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "#df = spark.read.csv(\"/home/jovyan/notebooks/final_project/whatsapp2/data/ml/ml_input\", header=True, inferSchema=True) no se pudo :(\n",
    "\n",
    "#Unir columnas de comentarios y compartidos\n",
    "df = df.withColumn(\"likes\", col(\"likes\").cast(\"int\")) \\\n",
    "       .withColumn(\"comments\", col(\"comments\").cast(\"int\")) \\\n",
    "       .withColumn(\"shares\", col(\"shares\").cast(\"int\"))\n",
    "\n",
    "#Columna viral (likes)\n",
    "df = df.withColumn(\"viral\", when(col(\"likes\") > 2000, 1).otherwise(0))\n",
    "\n",
    "#Eliminar valores nulos\n",
    "df = df.na.drop(subset=[\"likes\", \"comments\", \"shares\", \"viral\"])\n",
    "\n",
    "#Columnas originales (exportar CSV modelado)\n",
    "original_df = df.select(\"likes\", \"comments\", \"shares\", \"platform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd3468",
   "metadata": {},
   "source": [
    "### Ensamblar las características en una sola columna vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0177bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"likes\", \"comments\", \"shares\"], outputCol=\"features\")\n",
    "data_with_features = assembler.transform(df).withColumnRenamed(\"viral\", \"label\").select(\"label\", \"features\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127273e7",
   "metadata": {},
   "source": [
    "### Dividir los datos en conjuntos de entrenamiento y prueba: 80 % de datos de entrenamiento y 20 % de datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc903eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_with_features.randomSplit([0.8, 0.2], seed=57)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f1a7b1",
   "metadata": {},
   "source": [
    "### Mostrar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d95e28fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 06:29:16 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/05/11 06:29:16 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/05/11 06:29:17 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/05/11 06:29:18 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0| [1748.0,695.0,22.0]|\n",
      "|    0|  [517.0,631.0,20.0]|\n",
      "|    0| [1084.0,696.0,39.0]|\n",
      "|    1| [2592.0,191.0,40.0]|\n",
      "|    0| [124.0,498.0,197.0]|\n",
      "|    0|[1268.0,201.0,200.0]|\n",
      "|    1| [2124.0,636.0,42.0]|\n",
      "|    1|[3241.0,109.0,152.0]|\n",
      "|    0| [1718.0,284.0,38.0]|\n",
      "|    1| [3492.0,34.0,107.0]|\n",
      "|    0| [533.0,512.0,141.0]|\n",
      "|    1| [3577.0,517.0,15.0]|\n",
      "|    0|[1848.0,145.0,173.0]|\n",
      "|    0| [482.0,590.0,168.0]|\n",
      "|    1|  [2964.0,68.0,11.0]|\n",
      "|    1|[3121.0,379.0,108.0]|\n",
      "|    1| [2328.0,650.0,90.0]|\n",
      "|    1|[2553.0,146.0,183.0]|\n",
      "|    1|[2350.0,445.0,194.0]|\n",
      "|    1| [2584.0,145.0,88.0]|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 06:29:30 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/05/11 06:29:31 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/05/11 06:29:32 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "[Stage 2913:==============================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0| [1748.0,695.0,22.0]|\n",
      "|    0|  [517.0,631.0,20.0]|\n",
      "|    1| [2592.0,191.0,40.0]|\n",
      "|    0| [124.0,498.0,197.0]|\n",
      "|    0|[1268.0,201.0,200.0]|\n",
      "|    1| [2124.0,636.0,42.0]|\n",
      "|    1|[3241.0,109.0,152.0]|\n",
      "|    1| [3492.0,34.0,107.0]|\n",
      "|    0| [482.0,590.0,168.0]|\n",
      "|    0|[1848.0,145.0,173.0]|\n",
      "|    1|  [2964.0,68.0,11.0]|\n",
      "|    1|[3121.0,379.0,108.0]|\n",
      "|    1| [2328.0,650.0,90.0]|\n",
      "|    1|[2553.0,146.0,183.0]|\n",
      "|    1|[2350.0,445.0,194.0]|\n",
      "|    1| [2584.0,145.0,88.0]|\n",
      "|    0|  [361.0,81.0,157.0]|\n",
      "|    0|[1499.0,522.0,179.0]|\n",
      "|    0| [937.0,562.0,196.0]|\n",
      "|    1| [3280.0,220.0,30.0]|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Original Dataset\")\n",
    "data_with_features.show()\n",
    "\n",
    "# Print train dataset\n",
    "print(\"train set\")\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd5119",
   "metadata": {},
   "source": [
    "### Crear el modelo de regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb123f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba92c14",
   "metadata": {},
   "source": [
    "### Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38607bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 06:29:54 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:31:07 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:31:12 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/11 06:31:12 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/05/11 06:31:13 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:32:03 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:32:06 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:32:38 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:32:39 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:33:06 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:33:08 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:33:37 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:33:39 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:34:06 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:34:08 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:34:35 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:34:38 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:35:05 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:35:07 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:35:37 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:35:38 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:36:05 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:36:07 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:36:35 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:36:37 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:37:19 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0018771965205362678,0.0011581923465459184,-0.004835110164455652]\n"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "\n",
    "# Display model summary\n",
    "training_summary = lr_model.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a69d591",
   "metadata": {},
   "source": [
    "### Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "995a5518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 06:38:34 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/05/11 06:38:35 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/05/11 06:38:35 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/05/11 06:38:37 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "[Stage 2941:====================================================> (28 + 1) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|            features|prediction|         probability|\n",
      "+--------------------+----------+--------------------+\n",
      "| [1084.0,696.0,39.0]|       0.0|[0.76685711377311...|\n",
      "| [1718.0,284.0,38.0]|       0.0|[0.61605726855596...|\n",
      "| [533.0,512.0,141.0]|       0.0|[0.94937089264110...|\n",
      "| [3577.0,517.0,15.0]|       1.0|[0.03236027761848...|\n",
      "|    [76.0,578.0,9.0]|       0.0|[0.95582756042630...|\n",
      "|[2833.0,116.0,197.0]|       1.0|[0.34144334997760...|\n",
      "| [2504.0,82.0,137.0]|       1.0|[0.42800406737443...|\n",
      "|[2957.0,104.0,133.0]|       1.0|[0.23412032583566...|\n",
      "| [224.0,104.0,134.0]|       0.0|[0.98110947032679...|\n",
      "| [1618.0,83.0,227.0]|       0.0|[0.85902296577603...|\n",
      "| [1440.0,202.0,51.0]|       0.0|[0.75997370612232...|\n",
      "|[2582.0,332.0,112.0]|       1.0|[0.30009482549863...|\n",
      "| [248.0,408.0,180.0]|       0.0|[0.97758369251919...|\n",
      "|  [662.0,118.0,41.0]|       0.0|[0.93474257323464...|\n",
      "|  [76.0,405.0,186.0]|       0.0|[0.98418193668523...|\n",
      "| [543.0,361.0,166.0]|       0.0|[0.96114414895241...|\n",
      "|  [1416.0,295.0,1.0]|       0.0|[0.70017276375197...|\n",
      "|[1633.0,263.0,436.0]|       0.0|[0.92963552700285...|\n",
      "| [510.0,273.0,404.0]|       0.0|[0.98925902520871...|\n",
      "| [1910.0,145.0,13.0]|       0.0|[0.53806017586816...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use the trained model to make predictions on the test data\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"features\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c9357",
   "metadata": {},
   "source": [
    "### Evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2041294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 06:38:53 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:40:20 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:41:26 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/11 06:42:45 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "[Stage 2949:=============================================>      (505 + 2) / 581]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9676113360323887\n",
      "Precision: 0.9686316756620252\n",
      "Recall: 0.9676113360323886\n",
      "F1 Score: 0.9675439576863988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55adf9",
   "metadata": {},
   "source": [
    "El modelo nos da una conclusión de todas las métricas mayores al 96%. Esto demuestra que la aplicación es útil para predecir el potencial de viralidad de nuevas publicaciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3674d",
   "metadata": {},
   "source": [
    "### Exportar como CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e3b7abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 06:44:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/11 06:45:23 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "#Se unen los datos originales con los modelados\n",
    "original_with_features = assembler.transform(original_df).select(\"likes\", \"comments\", \"shares\", \"platform\", \"features\")\n",
    "#Unión entre las predicciones y los datos originales\n",
    "final_df = predictions.join(original_with_features, on=\"features\", how=\"left\")\n",
    "\n",
    "#Si = 1, es viral,si no, no es viral\n",
    "final_df = final_df.withColumn(\n",
    "    \"viral\",\n",
    "    when(final_df[\"prediction\"] == 1, \"Es viral\").otherwise(\"No es viral\")\n",
    ")\n",
    "\n",
    "#Ccolumnas originales y modeladas\n",
    "export_df = final_df.select(\"platform\", \"likes\", \"comments\", \"shares\", \"prediction\", \"viral\")\n",
    "#Exportar el CSV con pandas (Spark no me dejó por el espacio)\n",
    "export_df.toPandas().to_csv(\"/home/jovyan/notebooks/final_project/whatsapp2/data/predicciones_virales.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f349c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 06:48:12 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/11 06:49:22 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/05/11 06:49:25 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/05/11 06:49:28 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/05/11 06:49:31 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "[Stage 2956:================================================>       (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+----------+-----------+\n",
      "|platform|likes|comments|shares|prediction|      viral|\n",
      "+--------+-----+--------+------+----------+-----------+\n",
      "|Facebook| 1084|     696|    39|       0.0|No es viral|\n",
      "|Facebook| 1718|     284|    38|       0.0|No es viral|\n",
      "|Facebook|  533|     512|   141|       0.0|No es viral|\n",
      "|Facebook| 3577|     517|    15|       1.0|   Es viral|\n",
      "|Facebook|   76|     578|     9|       0.0|No es viral|\n",
      "+--------+-----+--------+------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "export_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b55f46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
