{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **Consumer** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de la conexión con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4c6cdd79-9dc1-4f44-8d55-d68651dfdc69;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.4 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.4 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 824ms :: artifacts dl 26ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;3.5.4 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4c6cdd79-9dc1-4f44-8d55-d68651dfdc69\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/19ms)\n",
      "25/05/01 05:17:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLStructuredStreaming-Kafka\") \\\n",
    "    .master(\"spark://80d04dce9402:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_lines = spark \\\n",
    "                .readStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"8e35f34f36db:9093\") \\\n",
    "                .option(\"subscribe\", \"kafka-spark-example\") \\\n",
    "                .load()\n",
    "\n",
    "kafka_lines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform binary data into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = kafka_lines.withColumn(\"value_str\", kafka_lines.value.cast(\"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del \"Sink\" del stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 05:17:18 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-115f409e-aa99-43a5-a2a3-35979a087549. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/01 05:17:18 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/01 05:17:18 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+-----+-----+---------+------+---------+-------------+---------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|value_str|\n",
      "+---+-----+-----+---------+------+---------+-------------+---------+\n",
      "+---+-----+-----+---------+------+---------+-------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 05:17:21 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3255 milliseconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|key |value                                                     |topic              |partition|offset|timestamp              |timestampType|value_str          |\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 36 30 2E 30 31 7D]|kafka-spark-example|0        |3998  |2025-05-01 05:17:24.974|0            |{\"price\": 94960.01}|\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 05:17:31 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 4205 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|key |value                                                     |topic              |partition|offset|timestamp              |timestampType|value_str          |\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 36 30 2E 30 31 7D]|kafka-spark-example|0        |3999  |2025-05-01 05:17:29.981|0            |{\"price\": 94960.01}|\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|key |value                                                     |topic              |partition|offset|timestamp              |timestampType|value_str          |\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 36 30 2E 30 31 7D]|kafka-spark-example|0        |4000  |2025-05-01 05:17:34.987|0            |{\"price\": 94960.01}|\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "|key |value                                            |topic              |partition|offset|timestamp              |timestampType|value_str       |\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 36 30 7D]|kafka-spark-example|0        |4001  |2025-05-01 05:17:39.991|0            |{\"price\": 94960}|\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "|key |value                                            |topic              |partition|offset|timestamp              |timestampType|value_str       |\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 36 30 7D]|kafka-spark-example|0        |4002  |2025-05-01 05:17:44.998|0            |{\"price\": 94960}|\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|key |value                                                     |topic              |partition|offset|timestamp              |timestampType|value_str          |\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 36 30 2E 30 31 7D]|kafka-spark-example|0        |4003  |2025-05-01 05:17:50.003|0            |{\"price\": 94960.01}|\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|key |value                                                     |topic              |partition|offset|timestamp              |timestampType|value_str          |\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 36 30 2E 30 31 7D]|kafka-spark-example|0        |4004  |2025-05-01 05:17:55.006|0            |{\"price\": 94960.01}|\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "|key |value                                            |topic              |partition|offset|timestamp              |timestampType|value_str       |\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 36 30 7D]|kafka-spark-example|0        |4005  |2025-05-01 05:18:00.011|0            |{\"price\": 94960}|\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|key |value                                                     |topic              |partition|offset|timestamp              |timestampType|value_str          |\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 36 30 2E 30 31 7D]|kafka-spark-example|0        |4006  |2025-05-01 05:18:05.017|0            |{\"price\": 94960.01}|\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "|key |value                                            |topic              |partition|offset|timestamp              |timestampType|value_str       |\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 36 30 7D]|kafka-spark-example|0        |4007  |2025-05-01 05:18:10.021|0            |{\"price\": 94960}|\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "|key |value                                            |topic              |partition|offset|timestamp              |timestampType|value_str       |\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 36 30 7D]|kafka-spark-example|0        |4008  |2025-05-01 05:18:15.027|0            |{\"price\": 94960}|\n",
      "+----+-------------------------------------------------+-------------------+---------+------+-----------------------+-------------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|key |value                                                     |topic              |partition|offset|timestamp              |timestampType|value_str          |\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 35 39 2E 39 39 7D]|kafka-spark-example|0        |4009  |2025-05-01 05:18:20.032|0            |{\"price\": 94959.99}|\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|key |value                                                     |topic              |partition|offset|timestamp              |timestampType|value_str          |\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 35 39 2E 36 35 7D]|kafka-spark-example|0        |4010  |2025-05-01 05:18:25.036|0            |{\"price\": 94959.65}|\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|key |value                                                     |topic              |partition|offset|timestamp              |timestampType|value_str          |\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 35 34 2E 35 35 7D]|kafka-spark-example|0        |4011  |2025-05-01 05:18:30.044|0            |{\"price\": 94954.55}|\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|key |value                                                     |topic              |partition|offset|timestamp              |timestampType|value_str          |\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 35 32 2E 33 38 7D]|kafka-spark-example|0        |4012  |2025-05-01 05:18:35.048|0            |{\"price\": 94952.38}|\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 16\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|key |value                                                     |topic              |partition|offset|timestamp              |timestampType|value_str          |\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 35 31 2E 36 39 7D]|kafka-spark-example|0        |4013  |2025-05-01 05:18:40.054|0            |{\"price\": 94951.69}|\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 17\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|key |value                                                     |topic              |partition|offset|timestamp              |timestampType|value_str          |\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "|NULL|[7B 22 70 72 69 63 65 22 3A 20 39 34 39 35 31 2E 36 39 7D]|kafka-spark-example|0        |4014  |2025-05-01 05:18:45.062|0            |{\"price\": 94951.69}|\n",
      "+----+----------------------------------------------------------+-------------------+---------+------+-----------------------+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m kafka_df \\\n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      3\u001b[0m                 \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m----> 9\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/pyspark/sql/streaming/query.py:219\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    216\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE_NOT_POSITIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    217\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_value\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(timeout)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    218\u001b[0m         )\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "query = kafka_df \\\n",
    "                .writeStream \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .trigger(processingTime='3 seconds') \\\n",
    "                .format(\"console\") \\\n",
    "                .option(\"truncate\", \"false\") \\\n",
    "                .start()\n",
    "\n",
    "query.awaitTermination(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/10 02:04:09 ERROR TaskSchedulerImpl: Lost executor 1 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 02:04:09 ERROR TaskSchedulerImpl: Lost executor 0 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 03:09:30 ERROR TaskSchedulerImpl: Lost executor 3 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 03:09:30 ERROR TaskSchedulerImpl: Lost executor 2 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 03:46:21 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 2184299 milliseconds\n",
      "25/04/10 03:57:42 ERROR TaskSchedulerImpl: Lost executor 5 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 03:57:42 ERROR TaskSchedulerImpl: Lost executor 4 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 04:07:38 ERROR TaskSchedulerImpl: Lost executor 7 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 04:07:38 ERROR TaskSchedulerImpl: Lost executor 6 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 04:15:43 ERROR TaskSchedulerImpl: Lost executor 9 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 04:15:43 ERROR TaskSchedulerImpl: Lost executor 8 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 04:47:18 ERROR TaskSchedulerImpl: Lost executor 11 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 04:47:18 ERROR TaskSchedulerImpl: Lost executor 10 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 07:40:31 ERROR TaskSchedulerImpl: Lost executor 12 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 07:40:31 ERROR TaskSchedulerImpl: Lost executor 13 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 09:04:43 ERROR TaskSchedulerImpl: Lost executor 14 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 09:04:43 ERROR TaskSchedulerImpl: Lost executor 15 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 09:16:51 ERROR TaskSchedulerImpl: Lost executor 17 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 09:16:51 ERROR TaskSchedulerImpl: Lost executor 16 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 09:58:27 ERROR TaskSchedulerImpl: Lost executor 19 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 09:58:27 ERROR TaskSchedulerImpl: Lost executor 18 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 10:17:19 ERROR TaskSchedulerImpl: Lost executor 20 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 10:17:19 ERROR TaskSchedulerImpl: Lost executor 21 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 10:31:50 ERROR TaskSchedulerImpl: Lost executor 22 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 10:31:50 ERROR TaskSchedulerImpl: Lost executor 23 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 11:33:25 ERROR TaskSchedulerImpl: Lost executor 25 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 11:33:25 ERROR TaskSchedulerImpl: Lost executor 24 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 12:05:18 ERROR TaskSchedulerImpl: Lost executor 26 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 12:05:18 ERROR TaskSchedulerImpl: Lost executor 27 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 12:31:51 ERROR TaskSchedulerImpl: Lost executor 29 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 12:31:51 ERROR TaskSchedulerImpl: Lost executor 28 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 12:45:44 ERROR TaskSchedulerImpl: Lost executor 31 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 12:45:44 ERROR TaskSchedulerImpl: Lost executor 30 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 12:59:18 ERROR TaskSchedulerImpl: Lost executor 32 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 12:59:18 ERROR TaskSchedulerImpl: Lost executor 33 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 13:37:42 ERROR TaskSchedulerImpl: Lost executor 34 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 13:37:42 ERROR TaskSchedulerImpl: Lost executor 35 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 13:51:30 ERROR TaskSchedulerImpl: Lost executor 37 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 13:51:30 ERROR TaskSchedulerImpl: Lost executor 36 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 13:56:42 ERROR TaskSchedulerImpl: Lost executor 38 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 13:56:42 ERROR TaskSchedulerImpl: Lost executor 39 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 14:24:40 ERROR TaskSchedulerImpl: Lost executor 40 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 14:24:40 ERROR TaskSchedulerImpl: Lost executor 41 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 15:24:00 ERROR TaskSchedulerImpl: Lost executor 43 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 15:24:00 ERROR TaskSchedulerImpl: Lost executor 42 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 15:37:49 ERROR TaskSchedulerImpl: Lost executor 45 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 15:37:49 ERROR TaskSchedulerImpl: Lost executor 44 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 16:00:10 ERROR TaskSchedulerImpl: Lost executor 47 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 16:00:10 ERROR TaskSchedulerImpl: Lost executor 46 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 16:16:36 ERROR TaskSchedulerImpl: Lost executor 49 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 16:16:36 ERROR TaskSchedulerImpl: Lost executor 48 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 16:33:14 ERROR TaskSchedulerImpl: Lost executor 50 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 16:33:14 ERROR TaskSchedulerImpl: Lost executor 51 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 16:43:33 ERROR TaskSchedulerImpl: Lost executor 52 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 16:43:33 ERROR TaskSchedulerImpl: Lost executor 53 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 18:08:30 ERROR TaskSchedulerImpl: Lost executor 54 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 18:08:30 ERROR TaskSchedulerImpl: Lost executor 55 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 18:27:40 ERROR TaskSchedulerImpl: Lost executor 57 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 18:27:40 ERROR TaskSchedulerImpl: Lost executor 56 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 18:43:45 ERROR TaskSchedulerImpl: Lost executor 58 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 18:43:45 ERROR TaskSchedulerImpl: Lost executor 59 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 19:19:42 ERROR TaskSchedulerImpl: Lost executor 61 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 19:19:42 ERROR TaskSchedulerImpl: Lost executor 60 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 19:41:08 ERROR TaskSchedulerImpl: Lost executor 63 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 19:41:08 ERROR TaskSchedulerImpl: Lost executor 62 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 19:58:02 ERROR TaskSchedulerImpl: Lost executor 65 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 19:58:02 ERROR TaskSchedulerImpl: Lost executor 64 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 20:37:04 ERROR TaskSchedulerImpl: Lost executor 66 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 20:37:04 ERROR TaskSchedulerImpl: Lost executor 67 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 20:50:25 ERROR TaskSchedulerImpl: Lost executor 68 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 20:50:25 ERROR TaskSchedulerImpl: Lost executor 69 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 21:38:27 ERROR TaskSchedulerImpl: Lost executor 70 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 21:38:27 ERROR TaskSchedulerImpl: Lost executor 71 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 21:42:10 ERROR TaskSchedulerImpl: Lost executor 72 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 21:42:10 ERROR TaskSchedulerImpl: Lost executor 73 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 21:58:42 ERROR TaskSchedulerImpl: Lost executor 75 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 21:58:42 ERROR TaskSchedulerImpl: Lost executor 74 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 22:58:47 ERROR TaskSchedulerImpl: Lost executor 76 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 22:58:47 ERROR TaskSchedulerImpl: Lost executor 77 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 23:09:47 ERROR TaskSchedulerImpl: Lost executor 78 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 23:09:47 ERROR TaskSchedulerImpl: Lost executor 79 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 23:36:08 ERROR TaskSchedulerImpl: Lost executor 80 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/10 23:36:08 ERROR TaskSchedulerImpl: Lost executor 81 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 00:15:15 ERROR TaskSchedulerImpl: Lost executor 83 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 00:15:15 ERROR TaskSchedulerImpl: Lost executor 82 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 00:31:37 ERROR TaskSchedulerImpl: Lost executor 84 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 00:31:37 ERROR TaskSchedulerImpl: Lost executor 85 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 00:38:03 ERROR TaskSchedulerImpl: Lost executor 87 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 00:38:03 ERROR TaskSchedulerImpl: Lost executor 86 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 01:26:19 ERROR TaskSchedulerImpl: Lost executor 88 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 01:26:19 ERROR TaskSchedulerImpl: Lost executor 89 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 01:35:10 ERROR TaskSchedulerImpl: Lost executor 91 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 01:35:10 ERROR TaskSchedulerImpl: Lost executor 90 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 01:47:56 ERROR TaskSchedulerImpl: Lost executor 92 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 01:47:56 ERROR TaskSchedulerImpl: Lost executor 93 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 02:14:51 ERROR TaskSchedulerImpl: Lost executor 95 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 02:14:51 ERROR TaskSchedulerImpl: Lost executor 94 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 02:18:45 ERROR TaskSchedulerImpl: Lost executor 96 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 02:18:45 ERROR TaskSchedulerImpl: Lost executor 97 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 02:24:49 ERROR TaskSchedulerImpl: Lost executor 99 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 02:24:49 ERROR TaskSchedulerImpl: Lost executor 98 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 02:48:32 ERROR TaskSchedulerImpl: Lost executor 101 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 02:48:32 ERROR TaskSchedulerImpl: Lost executor 100 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 03:22:32 ERROR TaskSchedulerImpl: Lost executor 103 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 03:22:32 ERROR TaskSchedulerImpl: Lost executor 102 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 03:43:18 ERROR TaskSchedulerImpl: Lost executor 105 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 03:43:18 ERROR TaskSchedulerImpl: Lost executor 104 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 04:06:32 ERROR TaskSchedulerImpl: Lost executor 107 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 04:06:32 ERROR TaskSchedulerImpl: Lost executor 106 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 04:23:16 ERROR TaskSchedulerImpl: Lost executor 109 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 04:23:16 ERROR TaskSchedulerImpl: Lost executor 108 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 04:45:34 ERROR TaskSchedulerImpl: Lost executor 111 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 04:45:34 ERROR TaskSchedulerImpl: Lost executor 110 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 04:51:08 ERROR TaskSchedulerImpl: Lost executor 112 on 172.24.0.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 04:51:08 ERROR TaskSchedulerImpl: Lost executor 113 on 172.24.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/04/11 06:27:46 WARN HeartbeatReceiver: Removing executor 114 with no recent heartbeats: 339615 ms exceeds timeout 120000 ms\n",
      "25/04/11 06:27:46 ERROR TaskSchedulerImpl: Lost executor 114 on 172.24.0.4: Executor heartbeat timed out after 339615 ms\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
