{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **K means model** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez\n",
    "\n",
    "**Integrantes**:\n",
    "- Lorena Ruelas Gaytán\n",
    "- Yael Alejandro Rodríguez Barreto\n",
    "- Ximena Isaac Horta\n",
    "- Alberto Renteria Camacho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_ID = \"91c5715ce4dd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Conexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/08 01:37:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLSpark-K-means\") \\\n",
    "    .master(f\"spark://{SPARK_ID}:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1580 Cols: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------+---------+------+--------------------+-------------+--------------------+\n",
      "| key|               value|  topic|partition|offset|           timestamp|timestampType|           value_str|\n",
      "+----+--------------------+-------+---------+------+--------------------+-------------+--------------------+\n",
      "|NULL|[7B 22 74 77 65 6...|tweet-2|        0|     4|2025-05-07 18:35:...|            0|{\"tweet_id\": \"5db...|\n",
      "|NULL|[7B 22 74 77 65 6...|tweet-2|        0|     5|2025-05-07 18:35:...|            0|{\"tweet_id\": \"d7b...|\n",
      "|NULL|[7B 22 74 77 65 6...|tweet-2|        0|     6|2025-05-07 18:35:...|            0|{\"tweet_id\": \"621...|\n",
      "|NULL|[7B 22 74 77 65 6...|tweet-2|        0|     7|2025-05-07 18:35:...|            0|{\"tweet_id\": \"707...|\n",
      "|NULL|[7B 22 74 77 65 6...|tweet-2|        0|     8|2025-05-07 18:35:...|            0|{\"tweet_id\": \"bc8...|\n",
      "|NULL|[7B 22 74 77 65 6...|tweet-2|        0|     9|2025-05-07 18:35:...|            0|{\"tweet_id\": \"738...|\n",
      "|NULL|[7B 22 74 77 65 6...|tweet-2|        0|    10|2025-05-07 18:36:...|            0|{\"tweet_id\": \"9d9...|\n",
      "|NULL|[7B 22 74 77 65 6...|tweet-2|        0|    11|2025-05-07 18:36:...|            0|{\"tweet_id\": \"df5...|\n",
      "|NULL|[7B 22 74 77 65 6...|tweet-2|        0|    12|2025-05-07 18:36:...|            0|{\"tweet_id\": \"2fb...|\n",
      "|NULL|[7B 22 74 77 65 6...|tweet-2|        0|    13|2025-05-07 18:36:...|            0|{\"tweet_id\": \"ba1...|\n",
      "+----+--------------------+-------+---------+------+--------------------+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/home/jovyan/notebooks/data/parquet/\"\n",
    "\n",
    "raw_df = spark.read \\\n",
    "    .parquet(data_path)\n",
    "\n",
    "print(\"Rows:\", raw_df.count(), \"Cols:\", len(raw_df.columns))\n",
    "raw_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1580 Cols: 11\n",
      "root\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- hashtags: string (nullable = true)\n",
      " |-- mentions: string (nullable = true)\n",
      " |-- retweet_count: integer (nullable = true)\n",
      " |-- favorite_count: integer (nullable = true)\n",
      " |-- reply_count: integer (nullable = true)\n",
      " |-- quote_count: integer (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+--------------------+--------------------+-------------+--------------+-----------+-----------+-----+\n",
      "|            tweet_id|user_id|           timestamp|                text|            hashtags|            mentions|retweet_count|favorite_count|reply_count|quote_count|views|\n",
      "+--------------------+-------+--------------------+--------------------+--------------------+--------------------+-------------+--------------+-----------+-----------+-----+\n",
      "|5dbb99f0-e6c9-4f1...|      2|2024-11-04T17:08:...|Distinctio ration...|                  []|[\"camila48\",\"sote...|          412|           259|          1|         25|10413|\n",
      "|d7b832b2-88a8-423...|      2|2025-02-09T22:14:...|Minima labore rep...|[\"magnam\",\"vel\",\"...|        [\"kmaestas\"]|          438|           165|         46|         36|15013|\n",
      "|6219028b-becf-4eb...|      2|2025-01-04T16:59:...|Est asperiores qu...|[\"consequatur\",\"s...| [\"esquiveljoaquin\"]|           53|           718|         88|         33|11177|\n",
      "|7075b200-1c9d-4a6...|      2|2024-05-10T16:53:...|Distinctio sequi ...| [\"debitis\",\"iusto\"]|    [\"salasmarisol\"]|          388|           660|         47|          8| 2728|\n",
      "|bc8d67a3-9710-40e...|      2|2024-08-04T03:39:...|Aliquid labore su...|[\"et\",\"veniam\",\"q...|[\"dulcegranados\",...|           38|           657|         60|         35| 9698|\n",
      "|7380d39d-91d1-451...|      2|2024-10-31T07:56:...|Quae commodi aspe...|         [\"maiores\"]|                  []|          115|           614|         96|         38| 1642|\n",
      "|9d9ee1f9-24d6-4c2...|      2|2025-01-17T04:49:...|Quo nostrum itaqu...|                  []|[\"laboylourdes\",\"...|          193|           744|         13|         42| 1988|\n",
      "|df5c3d87-ee43-425...|      2|2025-02-07T04:41:...|Occaecati repudia...|           [\"eaque\"]|                  []|          498|           956|        100|         49|18552|\n",
      "|2fbb3357-a420-4e7...|      2|2025-03-24T17:26:...|Provident molesti...|      [\"at\",\"magni\"]|[\"socorro39\",\"jer...|          449|           909|         16|         22| 6261|\n",
      "|ba127c98-840e-4ba...|      2|2024-08-29T09:31:...|Natus asperiores ...|[\"officia\",\"neces...|           [\"ivela\"]|          393|           593|         65|         11| 5040|\n",
      "+--------------------+-------+--------------------+--------------------+--------------------+--------------------+-------------+--------------+-----------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from equipo.spark_utils import SparkUtils\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "headers = [\n",
    "        (\"tweet_id\", \"string\"),\n",
    "        (\"user_id\", \"integer\"),\n",
    "        (\"timestamp\", \"string\"),\n",
    "        (\"text\", \"string\"),\n",
    "        (\"hashtags\", \"string\"),\n",
    "        (\"mentions\", \"string\"),\n",
    "        (\"retweet_count\", \"integer\"),\n",
    "        (\"favorite_count\", \"integer\"),\n",
    "        (\"reply_count\", \"integer\"),\n",
    "        (\"quote_count\", \"integer\"),\n",
    "        (\"views\", \"integer\")\n",
    "]\n",
    "\n",
    "schema = SparkUtils.generate_schema([(head[0], head[1]) for head in headers])\n",
    "\n",
    "tweets_df = raw_df.select(from_json(raw_df.value_str, schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "print(\"Rows:\", tweets_df.count(), \"Cols:\", len(tweets_df.columns))\n",
    "tweets_df.printSchema()\n",
    "tweets_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble the features into a single vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numeric_cols = [\n",
    "    \"retweet_count\", \"favorite_count\", \"reply_count\", \"quote_count\", \"views\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows:\", assembled_df.count(), \"Cols:\", len(assembled_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# KMeans at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "k_values = [2, 5, 10, 15, 20]\n",
    "kmeans = [KMeans().setK(k).setSeed(19) for k in k_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 01:45:04 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/08 01:45:04 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "[Stage 192:====>                                                   (1 + 1) / 12]\r"
     ]
    }
   ],
   "source": [
    "models = [k.fit(assembled_df) for k in kmeans]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [model.transform(assembled_df) for model in models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "for i in range(len(k_values)):\n",
    "    print(f\"\\n K values = {k_values[i]}\")\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    silhouette = evaluator.evaluate(predictions[i])\n",
    "    print(f\"Silhouette score: {silhouette}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k_values)):\n",
    "    print(f\"\\n K values = {k_values[i]}\")\n",
    "    print(\"Cluster Centers: \")\n",
    "    for center in models[i].clusterCenters():\n",
    "        print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
