{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b8fa74d",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **Documentación** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez\n",
    "\n",
    "**Team Members**:\n",
    "\n",
    "- Alberto Renteria Camacho\n",
    "- Lorena Ruelas Gaytán\n",
    "- Ximena Isaac Horta\n",
    "- Yael Alejandro Rodríguez Barreto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860ac28",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5250c46c",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "El presente proyecto tiene como objetivo principal demostrar las competencias adquiridas durante el curso de Procesamiento de Datos Masivos, mediante el diseño y construcción de una aplicación escalable que aprovecha el poder de procesamiento distribuido de PySpark. Esta aplicación está orientada al análisis de flujos de datos en tiempo real provenientes de redes sociales, en cumplimiento con las 5V del Big Data: volumen, velocidad, variedad, veracidad y valor.\n",
    "Entre las opciones de fuentes de datos sugeridas, se seleccionó \"Social Media Feeds\", enfocándose particularmente en la plataforma Twitter. Esta red social proporciona un flujo constante y diverso de datos públicos que pueden ser recolectados, procesados y analizados en tiempo real. La solución desarrollada se enfoca en construir una arquitectura robusta que permita recibir tweets, almacenarlos eficientemente y prepararlos para su análisis posterior con modelos de inteligencia artificial y herramientas de visualización como Power BI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f73156",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fdbf8a",
   "metadata": {},
   "source": [
    "## Definición del Problema\n",
    "El problema a resolver en este proyecto se centra en la necesidad de construir una arquitectura robusta y distribuida que permita recolectar, procesar y analizar flujos de datos provenientes de redes sociales en tiempo real, con la finalidad de extraer valor a partir de grandes volúmenes de información. La arquitectura propuesta se apoya en la integración de Apache Kafka, Apache Spark y modelos de aprendizaje automático, permitiendo cubrir tanto el procesamiento en tiempo real como el procesamiento por lotes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43598bca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743bc9a",
   "metadata": {},
   "source": [
    "## Arquitectura del sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bff19c",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; display: flex; justify-content: left; align-items: flex-start; margin-top: 20px;\">\n",
    "  <div style=\"width: 70%; padding-right: 40px; font-size: 16px; line-height: 1.6; text-align: justify;\">\n",
    "    <p>\n",
    "      La arquitectura del sistema se basa en un flujo distribuido de procesamiento de datos,\n",
    "      compuesto por tres etapas principales: <strong>generación de datos</strong>,\n",
    "      <strong>procesamiento en tiempo real</strong> y <strong>análisis por lotes</strong>.\n",
    "      Esta estructura permite simular y analizar publicaciones de redes sociales con tecnologías Big Data\n",
    "      y se ve reflejada en el diagrama.\n",
    "    </p>\n",
    "    <p>\n",
    "      En la primera etapa, ubicada en el archivo <strong>producers.ipynb</strong>, se utilizan productores Kafka \n",
    "      para enviar mensajes a un clúster distribuido. Cada productor está asociado a un tópico \n",
    "      (<em>tweet-1</em> a <em>tweet-4</em>) y genera un mensaje cada 1.5 segundos. Para simular tweets \n",
    "      de forma realista, se emplea la librería <strong>Faker</strong>, que permite crear contenido textual \n",
    "      sintético con estructura similar a publicaciones reales. Estos mensajes se envían de forma concurrente \n",
    "      mediante técnicas de <strong>multihilo</strong>.\n",
    "    </p>\n",
    "    <p>\n",
    "      En la segunda etapa, definida en <strong>consumer.ipynb</strong>, se configura un consumidor Kafka \n",
    "      conectado a <strong>Spark</strong>. Este módulo escucha los tópicos definidos, transforma los mensajes \n",
    "      recibidos en cadenas de texto y los almacena en formato <strong>Parquet</strong> en la ruta \n",
    "      <code>/home/jovyan/notebook/data/parquet</code>. Esta colección de archivos representa el <em>data lake</em> \n",
    "      del sistema, preparado para análisis posteriores.\n",
    "    </p>\n",
    "    <p>\n",
    "      La tercera etapa, implementada en <strong>k_means.ipynb</strong>, corresponde al análisis por lotes mediante \n",
    "      <strong>aprendizaje automático no supervisado</strong>. Se inicia una sesión Spark, se carga la información \n",
    "      desde los archivos Parquet y se extrae el campo <code>value_str</code>, que contiene el contenido textual \n",
    "      de los tweets. Este se convierte en vectores de características para alimentar un modelo de \n",
    "      <strong>KMeans</strong>, el cual agrupa los datos en clústeres según su similitud. Este proceso permite \n",
    "      segmentar el contenido y extraer patrones sin requerir datos etiquetados.\n",
    "    </p>\n",
    "    <p>\n",
    "      Esta arquitectura integra <strong>procesamiento en tiempo real</strong> (Kafka + Spark Streaming) con \n",
    "      <strong>análisis por lotes</strong> (Spark + Machine Learning), facilitando la gestión del flujo de datos \n",
    "      y preparando la información para ser visualizada en <strong>Power BI</strong>.\n",
    "    </p>\n",
    "  </div>\n",
    " <div style=\"width: 400px; display: flex; flex-direction: column; gap: 20px;\">\n",
    "    <img src=\"image.png\" alt=\"Diagrama del sistema\" style=\"max-width: 100%;\">\n",
    "    <img src=\"screenshot.png\" alt=\"Captura de pantalla\" style=\"max-width: 100%;\">\n",
    "  </div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d8f2b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82254000",
   "metadata": {},
   "source": [
    "## Justificación 5Vs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c75e5f",
   "metadata": {},
   "source": [
    "### Volumen\n",
    "El sistema maneja grandes volúmenes de datos al generar continuamente mensajes (tweets sintéticos) cada 1.5 segundos desde cuatro productores Kafka, lo que genera una carga constante en el clúster distribuido.\n",
    "Cada mensaje contiene texto con una estructura de publicación de red social, y es almacenado en formato Parquet, lo cual optimiza el almacenamiento y permite escalar el volumen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195732b",
   "metadata": {},
   "source": [
    "### Velocidad\n",
    "\n",
    "El sistema maneja grandes volúmenes de datos generados en tiempo real por cuatro productores Kafka, cada uno enviando un mensaje cada 1.5 segundos. Estos mensajes, que simulan publicaciones de redes sociales, son almacenados en formato Parquet.\n",
    "\n",
    "Para cuantificar el volumen de datos generado, se realizó una prueba en la que se monitoreó el tamaño total de los archivos producidos y el tiempo transcurrido. A continuación, se detalla el cálculo:\n",
    "\n",
    "- **Duración total de la ejecución:**\n",
    "  - Primer bloque de tiempo: 9 minutos 30 segundos → `9×60 + 30 = 570 segundos`\n",
    "  - Segundo bloque de tiempo: 4 minutos 45 segundos → `4×60 + 45 = 285 segundos`\n",
    "  - **Total:** `570 + 285 = 855 segundos`\n",
    "- **Tamaño total generado:** `4,120,851 bytes`\n",
    "\n",
    "**Cálculo del rendimiento medio:**\n",
    "\n",
    "(4,120,851 bytes) / (855 segundos) = 4,819.7 bytes/s\n",
    "\n",
    "\n",
    "El rendimiento medio es de aproximadamente **4819.7 bytes/s**.\n",
    "\n",
    "Este rendimiento indica que el sistema puede manejar un flujo sostenido de más de **4.8 KB por segundo**, y este volumen se incrementa proporcionalmente si se añaden más productores o se amplía el tiempo de ejecución.\n",
    "\n",
    "#### Estimación de volumen de datos procesados\n",
    "\n",
    "| # | Periodo de tiempo         | Datos procesados       |\n",
    "|---|---------------------------|-------------------------|\n",
    "| 0 | 1 Segundo                 | 4.82 KB                |\n",
    "| 1 | 1 Minuto (60 segundos)    | 289.18 KB              |\n",
    "| 2 | 1 Hora (3,600 segundos)   | 17.35 MB               |\n",
    "| 3 | 1 Día (86,400 segundos)   | 416.42 MB              |\n",
    "| 4 | 1 Año (31.5 millones s)   | 151.82 GB              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bde210",
   "metadata": {},
   "source": [
    "### Variety\n",
    "La aplicación demuestra el principio de Variety del Big Data al manejar datos semi-estructurados en tiempo real, generados en distintos formatos y gestionados a través de varias herramientas del ecosistema Big Data.\n",
    "Específicamente:\n",
    "-\tLos datos consisten en texto generado aleatoriamente mediante la librería Faker, simulando publicaciones en redes sociales con estructura realista y diversa.\n",
    "-\tEstos mensajes se envían de forma continua a través de Kafka, en forma de eventos semi-estructurados que permiten incorporar distintos campos dentro del mismo mensaje.\n",
    "-\tPosteriormente, los datos son procesados por Spark Structured Streaming y almacenados en formato Parquet, que conserva el esquema de los datos, facilitando el análisis por lotes y la compatibilidad con herramientas externas como Power BI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fd159",
   "metadata": {},
   "source": [
    "### Veracity\n",
    "El sistema garantiza la calidad y precisión de los datos a través de las siguientes estrategias:\n",
    "-\tGeneración controlada: El uso de la librería Faker asegura que los datos simulados tengan una estructura coherente y libre de errores tipográficos o inconsistencias semánticas.\n",
    "-\tValidación por Spark: Durante el procesamiento en tiempo real, Spark aplica validaciones automáticas de tipo de datos y formato. Cualquier mensaje que no cumpla con el esquema predefinido puede ser descartado o marcado para revisión.\n",
    "-\tPersistencia en formato Parquet: El uso del formato Parquet no solo optimiza el almacenamiento, sino que preserva la integridad del esquema, lo que reduce el riesgo de errores por manipulación posterior.\n",
    "-\tAgrupamiento por KMeans: La etapa de análisis por lotes puede detectar datos atípicos o inconsistentes al formar clústeres. Aquellos registros que no se agrupan correctamente podrían indicar anomalías a revisar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d208a14f",
   "metadata": {},
   "source": [
    "### Value\n",
    "El sistema genera y almacena datos que, aunque simulados, están estructurados de forma que permiten extraer valor práctico mediante análisis no supervisado. Las columnas almacenadas en los archivos Parquet alimentan un modelo de KMeans, que agrupa las publicaciones por similitud textual.\n",
    "Los principales beneficios obtenidos son:\n",
    "-\tSegmentación automática del contenido: El análisis permite descubrir patrones de similitud entre publicaciones sin necesidad de etiquetarlas manualmente.\n",
    "-\tIdentificación de temas o tendencias: Los clústeres pueden representar diferentes categorías de contenido, simulando cómo se podrían analizar conversaciones reales en redes sociales.\n",
    "-\tVisualización con Power BI: Los resultados del análisis por lotes se integran con Power BI, facilitando la exploración visual de los grupos, la frecuencia de aparición, y otros atributos derivados de los datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70d7fb3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6635085e",
   "metadata": {},
   "source": [
    "## Detalles de Implementación\n",
    "\n",
    "La arquitectura del sistema se diseñó bajo un enfoque de **procesamiento distribuido en tiempo real y por lotes**, utilizando tecnologías del ecosistema Big Data que permiten simular, capturar, procesar y analizar publicaciones tipo redes sociales.\n",
    "\n",
    "\n",
    "### Tecnologías Utilizadas\n",
    "\n",
    "- **Apache Kafka**  \n",
    "  Utilizado como sistema de mensajería distribuido para el envío y recepción de datos en tiempo real.  \n",
    "  Se configuraron múltiples tópicos (`tweet-1` a `tweet-4`) para recibir mensajes simulados desde productores concurrentes.\n",
    "\n",
    "- **PySpark (Spark Structured Streaming)**  \n",
    "  Utilizado para consumir los mensajes provenientes de Kafka en tiempo real.  \n",
    "  Se usó PySpark para transformar los datos, definir el esquema y almacenarlos en formato Parquet, lo que permite su posterior análisis.\n",
    "\n",
    "- **Formato Parquet**  \n",
    "  Seleccionado como formato de almacenamiento columnar por su eficiencia en lectura/escritura  \n",
    "  y su compatibilidad con sistemas analíticos como Power BI.\n",
    "\n",
    "- **Faker (Librería de Python)**  \n",
    "  Empleada para la generación de contenido textual aleatorio con estructura similar  \n",
    "  a publicaciones reales en redes sociales.\n",
    "\n",
    "- **KMeans (Machine Learning con PySpark MLlib)**  \n",
    "  Algoritmo no supervisado utilizado para analizar por lotes el contenido textual almacenado,  \n",
    "  con el fin de identificar grupos de mensajes similares sin necesidad de etiquetas manuales.\n",
    "\n",
    "\n",
    "### Decisiones de Diseño\n",
    "\n",
    "- **Simulación multihilo de productores Kafka**  \n",
    "  Se optó por usar hilos paralelos para cada productor, permitiendo enviar mensajes de forma concurrente  \n",
    "  a diferentes tópicos. Esto simula un entorno realista de múltiples fuentes de datos.\n",
    "\n",
    "- **Separación en notebooks independientes por etapa**  \n",
    "  El sistema está modularizado en notebooks para una mayor claridad y mantenimiento:\n",
    "  - `producers.ipynb`: generación y envío de mensajes.\n",
    "  - `consumer.ipynb`: ingestión y almacenamiento.\n",
    "  - `k_means.ipynb`: análisis por lotes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a595bf5e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964589ae",
   "metadata": {},
   "source": [
    "## Resultados y Evaluación\n",
    "\n",
    "### Funcionalidad de la Aplicación\n",
    "\n",
    "La aplicación cumple satisfactoriamente con los objetivos definidos:\n",
    "\n",
    "- Simular, procesar y analizar publicaciones tipo red social en un flujo continuo de datos.\n",
    "- Implementar una arquitectura que combina procesamiento en tiempo real (**Kafka + Spark Streaming**) con análisis por lotes (**Spark MLlib**).\n",
    "- Integrarse con herramientas de visualización como **Power BI** para permitir una interpretación accesible de los resultados.\n",
    "\n",
    "\n",
    "### Pruebas por Componente\n",
    "\n",
    "Cada componente del sistema fue probado de manera modular:\n",
    "\n",
    "- **Productores Kafka**  \n",
    "  Generan mensajes cada 1.5 segundos en tópicos distintos (`tweet-1` a `tweet-4`), simulando publicaciones sociales en tiempo real.\n",
    "\n",
    "- **Consumidor Spark**  \n",
    "  Lee los mensajes en tiempo real desde los tópicos de Kafka y los almacena exitosamente en archivos en formato **Parquet**, construyendo el *data lake* del sistema.\n",
    "\n",
    "- **Módulo de Machine Learning**  \n",
    "  Transforma el contenido textual en vectores de características y aplica un modelo de **KMeans**, agrupando los mensajes por similitud sin necesidad de etiquetas.\n",
    "\n",
    "\n",
    "### Evaluación del Agrupamiento\n",
    "\n",
    "Dado que se trata de un modelo no supervisado, no se calculan métricas clásicas como *precision*, *recall* o *accuracy*, ya que no se dispone de etiquetas reales.  \n",
    "En su lugar, la evaluación se centra en:\n",
    "\n",
    "- La **coherencia semántica** dentro de cada clúster.\n",
    "- La **utilidad práctica** del agrupamiento para detectar patrones o tendencias.\n",
    "\n",
    "\n",
    "### Dashboard de Resultados\n",
    "\n",
    "A continuación, se presenta el **dashboard de Power BI** que visualiza los resultados del análisis:\n",
    "\n",
    "# AQUÍ VA LA IMAGEN DEL POWERBI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa461d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21bdd56",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "A lo largo de este proyecto se desarrolló una solución basada en tecnologías Big Data capaz de simular, procesar y analizar publicaciones tipo redes sociales en un flujo continuo de datos. La integración de herramientas como Kafka, Spark Structured Streaming, almacenamiento en formato Parquet y un modelo de KMeans permitió construir un sistema escalable y adaptable a escenarios reales de procesamiento masivo de información.\n",
    "Uno de los principales logros fue el diseño modular del sistema en tres etapas claras: generación de datos, procesamiento en tiempo real y análisis por lotes. Esta separación permitió evaluar y optimizar cada fase de forma independiente, facilitando la validación funcional y analítica del flujo de datos. Además, la integración con Power BI brindó una forma clara y accesible de visualizar los resultados del análisis.\n",
    "Durante el desarrollo del proyecto, se profundizó en conceptos como el procesamiento distribuido, la gestión de flujos de datos en tiempo real y la aplicación de modelos de aprendizaje no supervisado. También se fortalecieron habilidades técnicas en áreas como programación concurrente y análisis de información de valor.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
