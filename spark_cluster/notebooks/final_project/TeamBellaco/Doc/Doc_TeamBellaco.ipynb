{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a06f8d8",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Carrera: Ingenieria de Sistemas Computacionales** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "\n",
    "**Proyecto Final**: Proyecto Team Bellaco\n",
    "\n",
    "**Fecha**:13/05/2025\n",
    "\n",
    "**Nombre del Estudiante**:David Abraham Naranjo Salgado, Benjamin Leonardo Zarate Solano y Angel David Cortes Pacheco\n",
    "\n",
    "**Profesor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84072674",
   "metadata": {},
   "source": [
    "# Introduction and Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47324fed",
   "metadata": {},
   "source": [
    "En este proyecto final de Procesamiento de Datos Masivos, desarrollamos una aplicación de análisis y recomendación de contenido en una plataforma de video streaming, inspirada en servicios como Netflix o YouTube. El objetivo fue simular un entorno real donde múltiples tipos de datos (usuarios, películas y series) se generan y procesan en tiempo real, permitiendo construir un sistema de recomendaciones personalizadas basado en los gustos y afinidades del usuario.\n",
    "\n",
    "Para lograrlo, se utilizaron las siguientes tecnologías clave:\n",
    "\n",
    "- **Kafka** para la generación y transmisión de datos en tiempo real, con tres productores simulando usuarios, series y películas.\n",
    "- **PySpark Structured Streaming** para el consumo de datos desde Kafka y su almacenamiento en archivos Parquet, formando un mini Data Lake.\n",
    "- **ALS ()** para construir un modelo de recomendación colaborativo basado en coincidencias de preferencias entre usuarios y contenido.\n",
    "- **Power BI** como herramienta de visualización de las recomendaciones generadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e8047",
   "metadata": {},
   "source": [
    "# System Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7defb4",
   "metadata": {},
   "source": [
    "La arquitectura de nuestra aplicación se basa en un flujo de procesamiento de datos en tiempo real, desde la generación de paquetes hasta la generacion de recomendaciones. Se compone de los siguientes componentes principales:\n",
    "\n",
    "1. **Producers Kafka**  \n",
    "   Tres productores independientes generan datos simulados:\n",
    "   - `Producer A`: genera información de usuarios (id, type, timestamp, nombre_usuario, fecha_suscripcion, generos, directores, actores, clasificacion, decadas_favoritas).\n",
    "   - `Producer B`: genera información de películas (id, type, timestamp, titulo, generos, directores, actores, clasificacion, decada_lanzamiento).\n",
    "   - `Producer C`: genera información de series (id, type, timestamp, titulo, generos, directores, actores, clasificacion, numero_temporadas, numero_episodios).\n",
    "\n",
    "2. **Apache Kafka**  \n",
    "   Cada productor publica en un tópico diferente dentro del clúster Kafka, permitiendo que los datos fluyan en paralelo.\n",
    "\n",
    "3. **PySpark Structured Streaming**  \n",
    "   Se utiliza para leer en tiempo real los mensajes de Kafka, aplicar un esquema estructurado, y almacenar los datos en formato Parquet, organizados por tópico, creando así un pequeño **Data Lake**.\n",
    "\n",
    "4. **Modelo de Recomendación (ALS)**  \n",
    "   A partir de los datos almacenados, se construye una tabla de afinidades con columnas como `userId - contentId - rating`, y se entrena un modelo de recomendaciones basado en ALS (Alternating Least Squares), propio de filtrado colaborativo.\n",
    "\n",
    "5. **Exportación CSV + Power BI**  \n",
    "   Las recomendaciones generadas por el modelo se exportan como archivo CSV, que luego se carga en Power BI para mostrar insights y recomendaciones personalizadas por usuario.\n",
    "\n",
    "---\n",
    "\n",
    "### Diagrama de Arquitectura\n",
    "\n",
    "<center> <img src=\"../img/Arquitecture.png\" alt=\"Arquitecture\" width=\"250\" height=\"400\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1770e73",
   "metadata": {},
   "source": [
    "# 5Vs Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449f41a2",
   "metadata": {},
   "source": [
    "A continuación se explica cómo nuestra aplicación cumple con las 5Vs del Big Data:\n",
    "\n",
    "## • Volume (Volumen)\n",
    "\n",
    "El sistema maneja grandes volúmenes de datos al simular una plataforma de streaming en la que cada productor genera mensajes ricos en contenido (listas de géneros, directores, actores, etc.). \n",
    "\n",
    "Tamaño promedio de cada registro en bytes:\n",
    "\n",
    "| Time Period              | Data Processed |\n",
    "|--------------------------|----------------|\n",
    "| 1 Second                 | 500 KB         |\n",
    "| 1 Minute (60 Seconds)    | 30 MB          |\n",
    "| 1 Hour (3,600 Seconds)   | 1.8 GB         |\n",
    "| 1 Day (86,400 Seconds)   | 43.2 GB        |\n",
    "| 1 Year (31.5M Seconds)   | 15.7 TB        |\n",
    "\n",
    "Con 3 productores enviando datos simultáneamente, el sistema puede alcanzar varios GB de datos procesados en pocas horas.\n",
    "\n",
    "---\n",
    "\n",
    "## • Velocity (Velocidad)\n",
    "\n",
    "El sistema fue diseñado para funcionar en tiempo real. Utilizamos **Kafka** para ingesta continua y **PySpark Structured Streaming** para procesamiento inmediato. La métrica `processedRowsPerSecond`, obtenida desde los `QueryListeners` de Spark, permite cuantificar el rendimiento del sistema en producción.\n",
    "\n",
    "---\n",
    "\n",
    "## • Variety (Variedad)\n",
    "\n",
    "Se trabajan tres tipos distintos de datos:\n",
    "- **Usuarios** (`type = \"user\"`): con campos como `nombre_usuario`, `generos`, `decadas_favoritas`, etc.\n",
    "- **Películas** (`type = \"movie\"`)\n",
    "- **Series** (`type = \"series\"`)\n",
    "\n",
    "Cada uno tiene estructuras diferentes y listas de preferencias o características únicas. El esquema utilizado en PySpark refleja esta variedad:\n",
    "\n",
    "```python\n",
    "StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"titulo\", StringType(), True),\n",
    "    StructField(\"nombre_usuario\", StringType(), True),\n",
    "    StructField(\"fecha_suscripcion\", StringType(), True),\n",
    "    StructField(\"generos\", ArrayType(StringType()), True),\n",
    "    StructField(\"directores\", ArrayType(StringType()), True),\n",
    "    StructField(\"actores\", ArrayType(StringType()), True),\n",
    "    StructField(\"clasificacion\", StringType(), True),\n",
    "    StructField(\"decada_lanzamiento\", StringType(), True),\n",
    "    StructField(\"numero_temporadas\", StringType(), True),\n",
    "    StructField(\"numero_episodios\", StringType(), True),\n",
    "    StructField(\"decadas_favoritas\", ArrayType(StringType()), True),\n",
    "])\n",
    "```\n",
    "\n",
    "## • Variety (Variedad)\n",
    "\n",
    "El uso de esquemas en PySpark permite asegurar que cada mensaje recibido desde Kafka cumpla con una estructura válida. Además, se filtran por campo type para evitar inconsistencias al procesar diferentes entidades (usuario, película o serie).\n",
    "\n",
    "## • Value (Valor)\n",
    "\n",
    "El valor de esta solución se refleja en las recomendaciones generadas por el modelo de Machine Learning. Estas recomendaciones personalizadas pueden visualizarse en Power BI y tienen aplicación directa para mejorar la experiencia del usuario en plataformas reales de streaming, impulsando el engagement y la fidelización.\n",
    "\n",
    "## • Code Quality (Calidad del Código)\n",
    "\n",
    "La calidad del código fue una prioridad durante el desarrollo del proyecto. Se abordaron los siguientes aspectos clave:\n",
    "\n",
    "### • Functionality:\n",
    "    La aplicación cumple con los requisitos planteados, integrando correctamente el manejo de datos en tiempo real mediante Kafka y Structured Streaming, así como el procesamiento por lotes para entrenamiento de modelos.\n",
    "\n",
    "### • Code Structure and Organization:\n",
    "    Todo el código fue organizado en módulos separados por responsabilidad (producers, streaming, model). Se utilizaron funciones auxiliares dentro de lib/ para mantener la limpieza del código. Se agregaron comentarios explicativos y se modularizó cada componente para facilitar mantenimiento y pruebas.\n",
    "\n",
    "### • Efficiency:\n",
    "    Las transformaciones fueron agrupadas antes de cualquier acción costosa en Spark. El uso de Structured Streaming permite procesar datos de forma eficiente y escalable. Las recomendaciones se filtran y explotan usando joins optimizados y acciones distribuidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95585be",
   "metadata": {},
   "source": [
    "# Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3de154",
   "metadata": {},
   "source": [
    "La solución fue desarrollada utilizando un enfoque modular, separando cada componente clave en carpetas específicas. A continuación se describe cada parte de la implementación.\n",
    "\n",
    "---\n",
    "\n",
    "## Estructura del Proyecto\n",
    "\n",
    "TeamBellaco/\n",
    "\n",
    "    structured_streaming_kafka.ipynb\n",
    "    recommendation_model.ipynb\n",
    "\n",
    "    recommendations/\n",
    "    final_recommendations.csv/\n",
    "\n",
    "    lib/ \n",
    "    data_gen.py\n",
    "    kafka_producer-a.ipynb\n",
    "    kafka_producer-b.ipynb\n",
    "    kafka_producer-c.ipynb\n",
    "    rating_gen.py\n",
    "    title_gen.py\n",
    "\n",
    "    /home/jovyan/data/\n",
    "    parquet_output/\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Generación de Datos (Kafka Producers)\n",
    "\n",
    "Se diseñaron tres producers en formato notebook (`ipynb`) que utilizan funciones del módulo `lib/raiting_gen.py`. Cada productor genera mensajes JSON en tiempo real con estructura diferente:\n",
    "\n",
    "- **Producer A**: usuarios (`type = \"user\"`)\n",
    "- **Producer B**: películas (`type = \"movie\"`)\n",
    "- **Producer C**: series (`type = \"series\"`)\n",
    "\n",
    "Cada mensaje es enviado a un tópico Kafka distinto: `producer1_topic`, `producer2_topic`, y `producer3_topic`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Procesamiento en Tiempo Real (PySpark Structured Streaming)\n",
    "\n",
    "El script `structured_streaming_kafka.ipynb` ubicado en `TeamBellaco/` consume datos desde los tres tópicos Kafka utilizando PySpark Structured Streaming. El esquema es explícitamente definido para validar los campos del JSON.\n",
    "\n",
    "Los datos se transforman y almacenan como archivos Parquet en la carpeta `\"/home/jovyan/data`, lo cual actúa como nuestro **mini Data Lake**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Preparación y Entrenamiento del Modelo de Recomendación (ALS)\n",
    "\n",
    "En el notebook `TeamBellaco/recommendation_model.ipynb` se realiza lo siguiente:\n",
    "\n",
    "1. Se leen los Parquet files generados por cada productor.\n",
    "2. Se separan los registros por tipo: usuarios vs. contenidos.\n",
    "3. Se generan pares `userId - contentId` según coincidencias en preferencias (géneros, actores, etc.).\n",
    "4. Se simula un valor de `rating` basado en afinidad entre usuario y contenido.\n",
    "5. Se entrena un modelo de **ALS (Alternating Least Squares)** de PySpark MLlib.\n",
    "6. Se generan recomendaciones personalizadas por usuario.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Exportación y Visualización (Power BI)\n",
    "\n",
    "Las recomendaciones generadas se exportan como un archivo CSV en la carpeta `recommendations/final_recommendations.csv/`. Este archivo contiene:\n",
    "\n",
    "- `userId`\n",
    "- `recommended_content`\n",
    "- `titulo`\n",
    "- `predicted_rating`\n",
    "\n",
    "Este CSV se utiliza para construir un dashboard de Power BI con visualizaciones interactivas.\n",
    "\n",
    "---\n",
    "\n",
    "## Tecnologías Utilizadas\n",
    "\n",
    "- **Apache Kafka**: plataforma de streaming de eventos para datos en tiempo real.\n",
    "- **PySpark Structured Streaming**: procesamiento distribuido de datos con esquemas definidos.\n",
    "- **PySpark MLlib**: motor de aprendizaje automático distribuido.\n",
    "- **Power BI**: visualización de datos y generación de dashboards.\n",
    "- **Docker (opcional)**: entorno de ejecución para Kafka/ZooKeeper y Spark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a20709",
   "metadata": {},
   "source": [
    "# Results and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05abdf24",
   "metadata": {},
   "source": [
    "A continuación se presentan los resultados obtenidos tras entrenar el modelo de recomendación y generar las predicciones para cada usuario.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluación del Modelo\n",
    "\n",
    "Se utilizó el modelo **ALS (Alternating Least Squares)** para realizar recomendaciones de contenido basado en afinidad entre usuarios y películas/series. Para evaluar su rendimiento, se utilizó la métrica **RMSE (Root Mean Squared Error)** sobre el conjunto de prueba (`test`).\n",
    "\n",
    "```python\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE:\", rmse)\n",
    "```\n",
    "RMSE del modelo: 3.6799\n",
    "\n",
    "## Recomendaciones Generadas\n",
    "\n",
    "<center> <img src=\"../img/Arquitecture.png\" alt=\"Arquitecture\" width=\"400\" height=\"400\"> </center>\n",
    "\n",
    "## Dashboard en Power BI\n",
    "\n",
    "<center> <img src=\"../img/Arquitecture.png\" alt=\"Arquitecture\" width=\"600\" height=\"400\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bdb20",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ca882",
   "metadata": {},
   "source": [
    "# 6. Conclusiones\n",
    "\n",
    "Este proyecto permitió simular y construir una solución completa de análisis y recomendación de contenido en tiempo real, utilizando tecnologías clave en el ecosistema de Big Data. A lo largo del desarrollo, se integraron correctamente componentes de ingesta, procesamiento distribuido, modelado de datos y visualización.\n",
    "\n",
    "- El uso de Apache Kafka permitió simular flujos de datos en tiempo real desde múltiples fuentes con distintos esquemas.\n",
    "- La implementación de **PySpark Structured Streaming permitió el procesamiento en paralelo de grandes volúmenes de datos con validación estructurada.\n",
    "- La creación de un mini Data Lake en formato Parquet facilitó la persistencia y análisis posterior de los datos generados.\n",
    "- El modelo ALS logró generar recomendaciones personalizadas basadas en coincidencias entre usuarios y características del contenido, con un RMSE aceptable en los resultados de prueba.\n",
    "- La visualización con Power BI demostró cómo los datos generados pueden transformarse en valor real para el usuario final.\n",
    "\n",
    "### Retos identificados:\n",
    "- Ajustar el esquema para cubrir diferentes tipos de contenido implicó validaciones adicionales.\n",
    "- Generar ratings realistas requirió diseñar una lógica de afinidad personalizada.\n",
    "- Asegurar la eficiencia de Spark al trabajar con datos en streaming y batch simultáneamente.\n",
    "\n",
    "### Posibles mejoras:\n",
    "- Incluir métricas de popularidad o visualizaciones históricas.\n",
    "- Incorporar feedback real del usuario (implícito o explícito) como refuerzo del modelo.\n",
    "- Escalar el sistema a múltiples nodos Spark y Kafka distribuidos.\n",
    "\n",
    "Este proyecto no solo consolida los conocimientos adquiridos en el curso, sino que también demuestra la aplicabilidad real de las tecnologías de Big Data en soluciones de recomendación personalizadas.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
