{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b750979e",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "## <center> **Final Proyect** </center>\n",
    "\n",
    "\n",
    "**Team**:\n",
    "- Luis Raul Acosta Mendoza \n",
    "- Samantha Abigail Quintero \n",
    "- Arturo Benajamin Vergara Romo\n",
    "    \n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98cfa09",
   "metadata": {},
   "source": [
    "## Introduction and Problem Definition \n",
    "\n",
    "This project aims to analyze real-time data from an e-commerce web application. Using Apache Kafka, user session data will be consumed in real time, then processed and stored in Parquet files by Apache Spark. The resulting dataset will be used to train a Machine Learning model that can predict user behavior within the application.\n",
    "\n",
    "The data captured from the application includes real-time information about each user session, such as the pages visited, seen products, user interactions within the diferent elements in the page, such as scroll, zoom, hover and the clicks performed. The goal of this project is to leverage this information to predict whether a user is likely to make a purchase in future sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317d430",
   "metadata": {},
   "source": [
    "## System Architecture\n",
    "The architecture of this project consists of three real-time producers, each writing data to a different Kafka topic. Each producer generates a distinct type of data: the first focuses solely on the page and product the user is viewing, the second captures all click events within the session, and the third generates data related to other user interactions.\n",
    "\n",
    "For data consumption, three Kafka consumers are used, each reading from its respective topic. The data is processed using a PySpark session with structured streaming. Initially, the JSON data is parsed and transformed into columns, and then another streaming pipeline writes the data into Parquet files, effectively creating a small data lake.\n",
    "\n",
    "To enable machine learning, a separate Spark session reads the Parquet files. The data is grouped by session, computing aggregations such as the total number of clicks and other user interactions. These aggregated features are used to train a model. A DecisionTreeClassifier from PySpark’s MLlib was chosen, using the btn_buy variable as the label, which indicates whether the user clicked the “buy” button during that session.\n",
    "\n",
    "Finally, the prediction results were visualized in a Power BI dashboard to make the insights more accessible and easier to interpret.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768d86a",
   "metadata": {},
   "source": [
    "## 5v Justification\n",
    "\n",
    "- Volume:\n",
    "The system is designed to handle a high volume of user interaction data generated by the e-commerce application in real time. Each Kafka producer sends multiple JSON records per second, including page views, click events, and interaction data. The following table shows the computed data growth:\n",
    "\n",
    "| Time Period | Proceced Data |\n",
    "|-------------|---------------|\n",
    "| 1 Second    | 2KB           |\n",
    "| 1 Minute    | 124KB         |\n",
    "| 1 Hour      | 7MB           |\n",
    "| 1 Day       | 179MB         |\n",
    "\n",
    "- Velocity:\n",
    "The architecture processes data in real time using Apache Kafka and Spark Structured Streaming. Data is ingested with low latency from the Kafka topics and processed on the fly to be stored in Parquet files. The system supports continuous streaming and writing with near real-time feedback, making it suitable for scenarios that require instant insights. Using the QueryListener form pySpark we could obtain the rows per second of our aplication, this is 11.8\n",
    "\n",
    "- Variety:\n",
    "Three different types of data are collected: page views, user interactions, and click events, also its a diferent schema for each data, for page views the schema was: user_id (string), session_id (string), page_url (string), referrer_url (string), category (string), price (float), timestamp (string), for the user interactions: user_id (string), session_id (string), interaction_type (string), page_url (string), category (string), price (float), details (string), timestamp (string) and finally for the click events: user_id (string), session_id (string), element_id (string), page_url (string), category (string), price (float), timestamp (string), x_coord (float), y_coord (float)\n",
    " The use of multiple data sources and structures reflects the system's ability to handle diverse data types in a unified processing pipeline.\n",
    "\n",
    "- Veracity:\n",
    "To ensure data reliability, the system uses Kafka’s message durability and Spark’s schema validation during JSON parsing. Invalid or malformed data is filtered out during the transformation stage, and structured formats like Parquet further enforce data integrity for downstream analysis and modeling.\n",
    "\n",
    "- Value:\n",
    "The collected and processed data is used to train a machine learning model that predicts user purchase behavior. These predictions are visualized in Power BI, providing actionable insights that can drive marketing strategies, user experience improvements, and business decisions. This demonstrates the value extracted from the raw data through analytics.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee62ab",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "### Technologies Used\n",
    "- Apache Kafka\n",
    "Kafka was used as the real-time messaging system to decouple the data producers from consumers. It provides high throughput, scalability, and durability, making it ideal for real-time event streaming.\n",
    "\n",
    "- PySpark (Apache Spark with Python API)\n",
    "PySpark was used for both real-time and batch data processing. Spark Structured Streaming handled streaming data from Kafka, while batch jobs were used for feature engineering and model training.\n",
    "\n",
    "- Parquet\n",
    "Parquet, a columnar storage format, was chosen for its performance benefits in analytical workloads. It supports efficient compression and encoding, which reduces storage size and speeds up query performance.\n",
    "\n",
    "- Power BI\n",
    "Power BI was used for creating interactive dashboards to visualize the predictions and insights generated by the machine learning model.\n",
    "\n",
    "### Design Choices\n",
    "\n",
    "- Three separate Kafka producers and topics were used to isolate the types of data: page views, click events, and other user interactions. This separation improves modularity, scalability, and makes it easier to track issues.\n",
    "\n",
    "\n",
    "- Storing data in Parquet format within a data lake enabled historical data analysis and reuse for training ML models without re-ingesting data.\n",
    "\n",
    "- Data was aggregated at the session level to capture meaningful behavioral patterns. This allowed the machine learning model to learn from combined features such as total clicks, number of interactions, and product views per session.\n",
    "\n",
    "\n",
    "- A DecisionTreeClassifier was chosen for its interpretability and fast training time, which is suitable for iterative development and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3474745",
   "metadata": {},
   "source": [
    "## Results and Evaluation\n",
    "\n",
    "After running the machine learning model, the following results were obtained:\n",
    "                                                                                \n",
    "- Accuracy: 0.7901234567901234\n",
    "                                                                                \n",
    "- Precision: 0.855921855921856\n",
    "                                                                                \n",
    "- Recall: 0.7901234567901234\n",
    "\n",
    "- F1 Score: 0.7960105968521116\n",
    "\n",
    "### Power BI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13244fbe",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project successfully implemented a real-time data processing pipeline for an e-commerce web application using Apache Kafka, PySpark, and Parquet. User session data—such as page views, click events, and interactions—was collected and processed to create a rich dataset for training a DecisionTreeClassifier aimed at predicting the likelihood of a user making a purchase. The pipeline combined real-time streaming and batch processing, enabling both immediate insights and long-term analysis. Aggregating data at the session level improved model performance by providing contextual behavioral features. The results were visualized using Power BI, offering accessible and actionable insights. Key learnings include the effectiveness of integrating streaming technologies with machine learning, the benefits of session-based feature engineering, and the practicality of using interpretable models like decision trees to support early-stage predictive analytics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
