{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b750979e",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "## <center> **Final Project (Website Activity) Documentation** </center>\n",
    "\n",
    "\n",
    "**Team**:\n",
    "- Luis Raul Acosta Mendoza \n",
    "- Samantha Abigail Quintero \n",
    "- Arturo Benajamin Vergara Romo\n",
    "    \n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98cfa09",
   "metadata": {},
   "source": [
    "## Introduction and Problem Definition \n",
    "\n",
    "This project aims to analyze real-time data from an e-commerce web application. Using Apache Kafka, the user session data will be consumed in real time, then processed and stored in Parquet files by Apache Spark. The resulting dataset will be used to train a Machine Learning model that can predict user behavior within the application.\n",
    "\n",
    "The data captured from the application includes real-time information about each user session, such as the pages visited, seen products, user interactions within the diferent elements in the page, such as scrolling, zooming, hovering and the clicks performed. The goal of this project is to leverage this information to predict whether a user is likely to make a purchase in future sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317d430",
   "metadata": {},
   "source": [
    "## System Architecture\n",
    "The architecture of this project consists of three real-time producers, each writing data to a different Kafka topic. Each producer generates a distinct type of data: the first focuses solely on the page and product the user is viewing, the second captures all click events within the session, and the third generates data related to other user interactions.\n",
    "\n",
    "For data consumption, three Kafka consumers are used, each reading from its corresponding topic. The data is processed using a PySpark session with structured streaming. Initially, the JSON data is parsed and transformed into columns, and then another streaming pipeline writes the data into Parquet files, effectively creating a small data lake.\n",
    "\n",
    "To enable machine learning, a separate Spark session reads the Parquet files. The data is grouped by session, computing aggregations such as the total number of clicks and other user interactions. These aggregated features are used to train a model. A DecisionTreeClassifier from PySpark’s MLlib was chosen, using the btn_buy variable as the label, which indicates whether the user clicked the “buy” button during that session.\n",
    "\n",
    "Finally, the prediction results were visualized in a Power BI dashboard to make the insights more accessible and easier to interpret.\n",
    "\n",
    "# <center> <img src=\"img/Arqui.png\" alt=\"ITESO\" width=\"480\" height=\"380\"> </center>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768d86a",
   "metadata": {},
   "source": [
    "## 5V Justification\n",
    "\n",
    "This system aligns with the five fundamental characteristics of big data: **Volume, Velocity, Variety, Veracity, and Value**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Volume**  \n",
    "The system is built to handle a high volume of user interaction data generated in real time by the e-commerce application. Each Kafka producer streams multiple JSON records per second, including page views, click events, and interaction data.\n",
    "\n",
    "The following table summarizes estimated data growth:\n",
    "\n",
    "| Time Period | Processed Data |\n",
    "|-------------|----------------|\n",
    "| 1 Second    | 2 KB           |\n",
    "| 1 Minute    | 124 KB         |\n",
    "| 1 Hour      | 7 MB           |\n",
    "| 1 Day       | 179 MB         |\n",
    "\n",
    "---\n",
    "\n",
    "### **Velocity**  \n",
    "Data is ingested and processed in real time using **Apache Kafka** and **Spark Structured Streaming**. Kafka ensures fast, durable message delivery, while Spark enables on-the-fly processing and storage in Parquet format. This architecture supports continuous, low-latency data streaming.\n",
    "\n",
    "Using PySpark’s `QueryListener`, we observed an average processing rate of **11.8 rows/second**, demonstrating the system’s capability to handle real-time insights.\n",
    "\n",
    "---\n",
    "\n",
    "### **Variety**  \n",
    "The system handles multiple data types, each with its own schema, allowing for diverse analytical use cases:\n",
    "\n",
    "1. **Page Views**\n",
    "2. **User Interactions**\n",
    "3. **Click Events**\n",
    "\n",
    "#### **Page Views Schema**\n",
    "- `user_id` (string)  \n",
    "- `session_id` (string)  \n",
    "- `page_url` (string)  \n",
    "- `referrer_url` (string)  \n",
    "- `category` (string)  \n",
    "- `price` (float)  \n",
    "- `timestamp` (string)\n",
    "\n",
    "#### **User Interactions Schema**\n",
    "- `user_id` (string)  \n",
    "- `session_id` (string)  \n",
    "- `interaction_type` (string)  \n",
    "- `page_url` (string)  \n",
    "- `category` (string)  \n",
    "- `price` (float)  \n",
    "- `details` (string)  \n",
    "- `timestamp` (string)\n",
    "\n",
    "#### **Click Events Schema**\n",
    "- `user_id` (string)  \n",
    "- `session_id` (string)  \n",
    "- `element_id` (string)  \n",
    "- `page_url` (string)  \n",
    "- `category` (string)  \n",
    "- `price` (float)  \n",
    "- `timestamp` (string)  \n",
    "- `x_coord` (float)  \n",
    "- `y_coord` (float)\n",
    "\n",
    "---\n",
    "\n",
    "### **Veracity**  \n",
    "To ensure data reliability:\n",
    "\n",
    "- **Kafka** ensures message durability and ordered delivery.\n",
    "- **Spark** enforces schema validation during JSON parsing.\n",
    "- Malformed or incomplete records are filtered out during transformation.\n",
    "- Writing to **Parquet**, a strongly typed format, enforces structural integrity for downstream processing.\n",
    "\n",
    "---\n",
    "\n",
    "### **Value**  \n",
    "The processed data is used to **train machine learning models** that predict user purchase behavior. Results are visualized using **Power BI**, enabling:\n",
    "\n",
    "- Data-driven marketing strategies  \n",
    "- Personalized user experience improvements  \n",
    "- Business decisions based on predictive insights  \n",
    "\n",
    "This demonstrates how raw, high-volume data can be transformed into **tangible business value**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee62ab",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "### Technologies Used\n",
    "- Apache Kafka\n",
    "Kafka was used as the real-time messaging system to decouple the data producers from consumers. It provides high throughput, scalability, and durability, making it ideal for real-time event streaming.\n",
    "\n",
    "- PySpark (Apache Spark with Python API)\n",
    "PySpark was used for both real-time and batch data processing. Spark Structured Streaming handled streaming data from Kafka, while batch jobs were used for feature engineering and model training.\n",
    "\n",
    "- Parquet\n",
    "Parquet, a columnar storage format, was chosen for its performance benefits in analytical workloads. It supports efficient compression and encoding, which reduces storage size and speeds up query performance.\n",
    "\n",
    "- Power BI\n",
    "Power BI was used for creating interactive dashboards to visualize the predictions and insights generated by the machine learning model.\n",
    "\n",
    "### Design Choices\n",
    "\n",
    "- Three separate Kafka producers and topics were used to isolate the types of data: page views, click events, and other user interactions. This separation improves modularity, scalability, and makes it easier to track issues.\n",
    "\n",
    "\n",
    "- Storing data in Parquet format within a data lake enabled historical data analysis and reuse for training ML models without re-ingesting data.\n",
    "\n",
    "- Data was aggregated at the session level to capture meaningful behavioral patterns. This allowed the machine learning model to learn from combined features such as total clicks, number of interactions, and product views per session.\n",
    "\n",
    "\n",
    "- A DecisionTreeClassifier was chosen for its interpretability and fast training time, which is suitable for iterative development and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3474745",
   "metadata": {},
   "source": [
    "## Results and Evaluation\n",
    "\n",
    "After running the machine learning model, the following results were obtained:\n",
    "                                                                                \n",
    "- Accuracy: 0.9130434782608695\n",
    "                                                                                \n",
    "- Precision: 0.922572960095295\n",
    "                                                                                \n",
    "- Recall: 0.9130434782608696\n",
    "\n",
    "- F1 Score: 0.9080025204788911\n",
    "\n",
    "### Power BI\n",
    "\n",
    "# <center> <img src=\"img/PowerBi_report.png\" alt=\"report\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13244fbe",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project successfully implemented a real-time data processing pipeline for an e-commerce web application using **Apache Kafka**, **PySpark**, and **Parquet**. User session data—including page views, click events, and interactions—was collected and processed to build a rich dataset for training a **DecisionTreeClassifier** that predicts the likelihood of a user making a purchase.\n",
    "\n",
    "The pipeline combines **real-time streaming** with **batch processing**, enabling both instant feedback and long-term analytics. By aggregating data at the **session level**, the model benefited from richer behavioral context, resulting in improved predictive performance.\n",
    "\n",
    "Insights were visualized using **Power BI**, making the results accessible and actionable for final users or stakeholders.\n",
    "\n",
    "### Key Learnings:\n",
    "- The integration of **streaming technologies** with machine learning enables real-time, scalable analytics.\n",
    "- **Session-based feature engineering** enhances model accuracy by capturing user behavior over time.\n",
    "- **Interpretable models** like decision trees are effective for early-stage predictive tasks, providing transparency and ease of communication with non-technical teams.\n",
    "\n",
    "This project demonstrates the practical value of unifying data engineering and data science workflows to drive informed business decisions in real time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
