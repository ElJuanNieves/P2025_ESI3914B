{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71972a84",
   "metadata": {},
   "source": [
    "# <center> <img src=\"./img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Proyecto Final: Website Activity** </center>\n",
    "---\n",
    "\n",
    "<center>\n",
    "\n",
    "**Equipo McQueen**: Marco Albanese, Vicente Siloe\n",
    "\n",
    "**Carrera**: Ingeniería en Sistemas Computacionales\n",
    "\n",
    "**Fecha**: 13 de mayo del 2025\n",
    "\n",
    "**Profesor**: Pablo Camarillo Ramirez\n",
    "\n",
    "</center>\n",
    "\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfebc990",
   "metadata": {},
   "source": [
    "### **Introducción y definición del problema**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd1d95",
   "metadata": {},
   "source": [
    "El procesamiento de datos en aplicaciones es esencial hoy en día para una multitud de usos y propósitos que proveen beneficios tanto a usuarios como empresas. En este proyecto, creado al culminar el curso de \"Procesamiento de datos masivos\", se pretende crear una aplicación que integre los conocimientos adquiridos en dicho curso. A través de una *pipeline* de *Big Data*, se busca producir, consumir, analizar y procesar datos de la actividad (tráfico) observada en una aplicación web.\n",
    "\n",
    "Los datos involucrados representan:\n",
    "- Interacciones de usuario\n",
    "- Páginas visitadas\n",
    "- Clicks\n",
    "- Dispositivos utilizados\n",
    "- Etc.\n",
    "\n",
    "Asimismo, se entrena un modelo de aprendizaje automático (*machine learning*) en base a los datos recolectados para predecir comportamientos de usuario, utilizando regresión logística. Los resultados de este modelo se presentan al usuario final a través de un *dashboard* de visualización de datos.\n",
    "\n",
    "Para la realización de este proyecto, se utilizaron tecnologías como **Jupyter Notebooks**, **Python**, **PySpark**, **Apache Kafka** y **Power BI**. En secciones posteriores, se describe con mayor detalle la arquitectura, implementación y funcionamiento del sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984bd6e9",
   "metadata": {},
   "source": [
    "### **Arquitectura del sistema**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48b7c9",
   "metadata": {},
   "source": [
    "La arquitectura del sistema es la siguiente:\n",
    "\n",
    "1. Productores:\n",
    "2. Consumidor:\n",
    "3. Spark\n",
    "4. Modelo de aprendizaje máquina (Machine Learning)\n",
    "5. Visualización de datos:\n",
    "\n",
    "A continuación se presenta un diagrama visual de dicha arquitectura:\n",
    "\n",
    "\n",
    "# <img src=\"./img/system_architecture.png\" alt=\"System Architecture\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337cf66",
   "metadata": {},
   "source": [
    "### **Justificación de las 5Vs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70182f5a",
   "metadata": {},
   "source": [
    "- Volumen:\n",
    "- Velocidad:\n",
    "- Variedad:\n",
    "- Veracidad:\n",
    "- Valor:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae6570",
   "metadata": {},
   "source": [
    "### **Detalles de implementación**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63e84b",
   "metadata": {},
   "source": [
    "Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15fb8c",
   "metadata": {},
   "source": [
    "### **Resultados y evaluación**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237061f4",
   "metadata": {},
   "source": [
    "Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baea28ca",
   "metadata": {},
   "source": [
    "### **Conclusión**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b300474f",
   "metadata": {},
   "source": [
    "Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b00724",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367fbc33",
   "metadata": {},
   "source": [
    "### **Código: Spark + Kafka**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c132e4c",
   "metadata": {},
   "source": [
    "Previo a la ejecución del siguiente código, los contenedores de Docker para el cluster de Spark y el cluster de Kafka deben estar activos.\n",
    "\n",
    "Para ello, se puede utilizar el comando:\n",
    "\n",
    "```\n",
    "docker compose up\n",
    "```\n",
    "\n",
    "En los directorios `spark_cluster` y `kafka_cluster`. Notesé que se requerirán distintos IDs de contenedores en los bloques de código pertinentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f449c",
   "metadata": {},
   "source": [
    "#### Encontrar instalación de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290cbc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2bfa8",
   "metadata": {},
   "source": [
    "#### Creación de la conexión con el cluster de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25653e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Final-Project-Equipo-McQueen\") \\\n",
    "    .master(\"spark://f8e470126b03:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05cc495",
   "metadata": {},
   "source": [
    "#### Creación de tópicos de Kafka\n",
    "\n",
    "Los siguientes comandos deben ser ejecutados en la terminal para que los productores puedan enviar mensajes a los tópicos:\n",
    "\n",
    "```\n",
    "docker exec -it <kafka container id> /opt/kafka/bin/kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic producer-a\n",
    "\n",
    "docker exec -it <kafka container id> /opt/kafka/bin/kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic producer-b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca9b85",
   "metadata": {},
   "source": [
    "#### Ejecución de productores\n",
    "\n",
    "Para inicializar a los productores, ejecute los respectivos scripts de Python bajo el directorio `spark_cluster/notebooks/lib/equipo_mcqueen/` de la siguiente manera:\n",
    "\n",
    "`python producer-a.py`\n",
    "\n",
    "`python producer-b.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3ca3bd",
   "metadata": {},
   "source": [
    "#### Creación del Kafka stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784bb33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_lines = spark \\\n",
    "                .readStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"9bc654f7b2fc:9093\") \\\n",
    "                .option(\"subscribe\", \"producer-a\", \"producer-b\") \\\n",
    "                .load()\n",
    "\n",
    "kafka_lines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be47f3fb",
   "metadata": {},
   "source": [
    "#### Transformar datos en cadenas (strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = kafka_lines.withColumn(\"value_str\", kafka_lines.value.cast(\"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f5206e",
   "metadata": {},
   "source": [
    "#### Creación del query (sink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = kafka_df \\\n",
    "                .writeStream \\\n",
    "                .trigger(processingTime=\"3 seconds\") \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .format(\"parquet\") \\\n",
    "                .option(\"path\", \"/home/jovyan/notebooks/data/\") \\\n",
    "                .option(\"checkpointLocation\", \"/home/jovyan/checkpoint\") \\\n",
    "                .option(\"truncate\", \"false\") \\\n",
    "                .start()\n",
    "\n",
    "query.awaitTermination(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9353ec2",
   "metadata": {},
   "source": [
    "#### Terminación de query y context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8df6d",
   "metadata": {},
   "source": [
    "### **Código: Machine Learning**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5262ac63",
   "metadata": {},
   "source": [
    "#### Creación de la conexión con el cluster de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Equipo-McQueen-Logistic-Regression\") \\\n",
    "    .master(\"spark://078b2e28e517:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3296cc5e",
   "metadata": {},
   "source": [
    "#### Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafb618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afc6b09c",
   "metadata": {},
   "source": [
    "#### Creación de vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a80a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5fc4aa",
   "metadata": {},
   "source": [
    "#### División de datos para entrenamiento y pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a249d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90377422",
   "metadata": {},
   "source": [
    "#### Creación y entrenamiento del modelo (regresión logística)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a95e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d2a97",
   "metadata": {},
   "source": [
    "#### Evaluación y predicciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb99788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ada9dd20",
   "metadata": {},
   "source": [
    "### **Visualización de datos: Power BI**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fd8fa",
   "metadata": {},
   "source": [
    "#### Conversión a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ab952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f99be21",
   "metadata": {},
   "source": [
    "#### Dashboard de ejemplo\n",
    "\n",
    "Placeholder"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
