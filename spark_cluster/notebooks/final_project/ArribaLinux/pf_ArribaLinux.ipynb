{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **Procesamiento de Datos en Redes Sociales** </center>\n",
    "\n",
    "---\n",
    "\n",
    "**Proyecto Final**\n",
    "\n",
    "**Fecha**: 13 mayo 2025\n",
    "\n",
    "**Nombre del Equipo**: Arriba Linux\n",
    "\n",
    "**Integrantes del Equipo**: Tirzah Peniche Barba / Ana Cristina Luna Arellano / Juan Pedro Bihouet\n",
    "\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**INTRODUCCIÓN Y DEFINICIÓN DEL PROBLEMA**\n",
    "\n",
    "En la actualidad, las redes sociales se han consolidado como una fuente inagotable de datos generados en tiempo real por millones de usuarios en todo el mundo. Plataformas como Twitter e Instagram concentran interacciones constantes que reflejan opiniones, emociones, tendencias de consumo, comportamientos sociales y eventos relevantes en tiempo real. Aprovechar este flujo de información puede representar una ventaja estratégica para diversos sectores, como el marketing, la investigación social, la gestión de crisis, o incluso la predicción de fenómenos colectivos.\n",
    "\n",
    "El objetivo de este proyecto es desarrollar una solución de análisis en tiempo real de datos provenientes de redes sociales, utilizando tecnologías de procesamiento distribuido como Apache Kafka y Apache Spark. La aplicación simula un entorno donde se generan y transmiten publicaciones de redes sociales de manera continua, con el fin de analizarlas en tiempo real para extraer métricas de interés, como usuarios más activos, mensajes más populares, tendencias por frecuencia de palabras y patrones de interacción.\n",
    "\n",
    "El problema que se busca abordar es cómo gestionar, procesar y extraer valor de grandes volúmenes de datos no estructurados y en constante flujo. Las empresas y organizaciones frecuentemente enfrentan el reto de recibir información en tiempo real, pero carecen de mecanismos eficientes para procesarla sin demoras, asegurando al mismo tiempo la veracidad de los datos y obteniendo resultados útiles y accionables.\n",
    "\n",
    "Para abordar este reto, se ha diseñado una arquitectura basada en el ecosistema Big Data que permite el procesamiento en tiempo real (streaming), el manejo de grandes volumenes de información (escalabilidad), la integración de fuentes de datos diversas (variedad). La solución se contruye a partir de productores que simulan publicaciones de Twitter e Instagram, enviadas mediante Kafka a distintos tópicos. Spark Structured Streaming actúa como el motor de procesamiento que consume, transforma y analiza estos flujos, permitiendo realizar cálculos y visualizaciones de manera inmediata.\n",
    "\n",
    "Este proyecto busca demostrar cómo una arquitectura moderna de Big Data puede ser implementada para enfrentar los desafíos de las 5Vs del Big Data: Volumen, Velocidad, Veracidad y Valor. A través de este desarrollo, se pretende no solo evidenciar el funcionamiento técnico del sistema, sino también mostrar el valor práctica de contar con herramientas de análisis en tiempo real en un mundo cada vez más impulsado por los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARQUITECTURA DEL SISTEMA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System Architecture: Detailed description of the architecture used\n",
    "(how all components are connected), including diagrams and expla-\n",
    "nations of the real-time and batch processing components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5V JUSTIFICACIÓN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5Vs Justification: Brief explanation of how the application meets\n",
    "the 5Vs of Big Data:\n",
    "\n",
    "– Volume: How the system handles large data volumes. Each\n",
    "team needs to compute the size of each produced record to to\n",
    "this analysis. Take as example the following Data Growth shown\n",
    "in the following table:\n",
    "\n",
    "– Velocity: The system’s ability to process streaming data in real-\n",
    "time. The performance can be obtained by using the processedRowsPerSecond info obtained from the event progress\n",
    "data (using QueryListeners).\n",
    "\n",
    "– Variety: In this section, the report should include a description\n",
    "of the schema that the application is handling. The schema\n",
    "definition consists of the list of column names and data types\n",
    "consumed by the PySpark application. The schema will depend\n",
    "on the specific application chosen.\n",
    "\n",
    "– Veracity: How the system ensures the quality and accuracy of\n",
    "the data.\n",
    "\n",
    "– Value: The practical insights or benefits of the columns stored\n",
    "in the Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creacion de la conexión con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una sesión de Spark con el paquete de Kafka incluido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Arriba-Linux-Proyecto-Final\") \\\n",
    "    .master(\"spark://ac7f0d7e8e91:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el stream desde Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace la conexión a Kafka para escuchar los dos topicos. Kafka entrega los datos en formato binario, por lo que se debe convertirlos. Se crearon los tópicos directamente en la consola de kafka para ahí subscribirse a la lectura de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_lines = spark \\\n",
    "                .readStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"78a305ddc318:9093\") \\\n",
    "                .option(\"subscribePattern\", \".*_topic\") \\\n",
    "                .option(\"startingOffsets\", \"latest\") \\\n",
    "                .load()\n",
    "\n",
    "kafka_lines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformar los datos a columnas de strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usa .withColumn para castear el string y luego descomponerlo para obtener colúmnas útiles. Ya que todo se está usando en strings, se tomó la decisión de usar cast(\"int\") en el único dato diferente para simplificar el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "kafka_df = kafka_lines.withColumn(\"value_str\", kafka_lines.value.cast(\"string\"))\n",
    "\n",
    "social_df = kafka_df.select(\n",
    "    split(col(\"value_str\"), \" \\\\| \").alias(\"fields\")\n",
    ").select(\n",
    "    col(\"fields\")[0].alias(\"timestamp\"),\n",
    "    col(\"fields\")[1].alias(\"platform\"),\n",
    "    col(\"fields\")[2].alias(\"user\"),\n",
    "    col(\"fields\")[3].alias(\"text\"),\n",
    "    col(\"fields\")[4].cast(\"int\").alias(\"likes\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escribir el stream en formato Parquet (Data Lake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se guarda el resultado del stream como archivos Parquet en un directorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 02:57:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/13 02:57:50 WARN StreamingQueryManager: Stopping existing streaming query [id=736cf695-3fca-4dd5-bfa1-271f4661eafd, runId=c7bc72b0-c477-49ae-a193-05f8ae8cda5a], as a new run is being started.\n",
      "25/05/13 02:57:58 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/13 02:57:58 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 7517 milliseconds\n"
     ]
    }
   ],
   "source": [
    "query = social_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/home/jovyan/notebooks/datalake/social_logs/\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/jovyan/notebooks/datalake/_checkpoints/\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+-------------------------+-----+\n",
      "|timestamp          |platform |user   |text                     |likes|\n",
      "+-------------------+---------+-------+-------------------------+-----+\n",
      "|2025-05-13 02:23:21|instagram|user_8 |My heart is full         |1939 |\n",
      "|2025-05-13 02:23:21|instagram|user_25|Look at this outfit!     |2268 |\n",
      "|2025-05-13 02:23:21|instagram|user_37|Throwback to last summer |2110 |\n",
      "|2025-05-13 02:23:21|instagram|user_43|So in love with this meal|2920 |\n",
      "|2025-05-13 02:23:21|instagram|user_10|Morning vibes            |80   |\n",
      "|2025-05-13 02:23:21|instagram|user_39|Besties forever          |346  |\n",
      "|2025-05-13 02:23:21|instagram|user_50|This place is magical    |1301 |\n",
      "|2025-05-13 02:23:21|instagram|user_21|Beach days are the best  |1600 |\n",
      "|2025-05-13 02:23:21|instagram|user_13|My heart is full         |755  |\n",
      "|2025-05-13 02:23:21|instagram|user_24|Throwback to last summer |4326 |\n",
      "+-------------------+---------+-------+-------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/home/jovyan/notebooks/datalake/social_logs/\")\n",
    "df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
