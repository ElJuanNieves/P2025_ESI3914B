{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **Procesamiento de Datos en Redes Sociales** </center>\n",
    "\n",
    "---\n",
    "\n",
    "**Proyecto Final**\n",
    "\n",
    "**Fecha**: 13 mayo 2025\n",
    "\n",
    "**Nombre del Equipo**: Arriba Linux\n",
    "\n",
    "**Integrantes del Equipo**: Tirzah Peniche Barba / Ana Cristina Luna Arellano / Juan Pedro Bihouet\n",
    "\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creacion de la conexión con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una sesión de Spark con el paquete de Kafka incluido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Arriba-Linux-Proyecto-Final\") \\\n",
    "    .master(\"spark://ac7f0d7e8e91:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el stream desde Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace la conexión a Kafka para escuchar los dos topicos. Kafka entrega los datos en formato binario, por lo que se debe convertirlos. Se crearon los tópicos directamente en la consola de kafka para ahí subscribirse a la lectura de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_lines = spark \\\n",
    "                .readStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"78a305ddc318:9093\") \\\n",
    "                .option(\"subscribePattern\", \".*_topic\") \\\n",
    "                .option(\"startingOffsets\", \"latest\") \\\n",
    "                .load()\n",
    "\n",
    "kafka_lines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformar los datos a columnas de strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usa .withColumn para castear el string y luego descomponerlo para obtener colúmnas útiles. Ya que todo se está usando en strings, se tomó la decisión de usar cast(\"int\") en el único dato diferente para simplificar el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "kafka_df = kafka_lines.withColumn(\"value_str\", kafka_lines.value.cast(\"string\"))\n",
    "\n",
    "social_df = kafka_df.select(\n",
    "    split(col(\"value_str\"), \" \\\\| \").alias(\"fields\")\n",
    ").select(\n",
    "    col(\"fields\")[0].alias(\"timestamp\"),\n",
    "    col(\"fields\")[1].alias(\"platform\"),\n",
    "    col(\"fields\")[2].alias(\"user\"),\n",
    "    col(\"fields\")[3].alias(\"text\"),\n",
    "    col(\"fields\")[4].cast(\"int\").alias(\"likes\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escribir el stream en formato Parquet (Data Lake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se guarda el resultado del stream como archivos Parquet en un directorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 02:57:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/13 02:57:50 WARN StreamingQueryManager: Stopping existing streaming query [id=736cf695-3fca-4dd5-bfa1-271f4661eafd, runId=c7bc72b0-c477-49ae-a193-05f8ae8cda5a], as a new run is being started.\n",
      "25/05/13 02:57:58 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/13 02:57:58 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 7517 milliseconds\n"
     ]
    }
   ],
   "source": [
    "query = social_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/home/jovyan/notebooks/datalake/social_logs/\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/jovyan/notebooks/datalake/_checkpoints/\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+-------------------------+-----+\n",
      "|timestamp          |platform |user   |text                     |likes|\n",
      "+-------------------+---------+-------+-------------------------+-----+\n",
      "|2025-05-13 02:23:21|instagram|user_8 |My heart is full         |1939 |\n",
      "|2025-05-13 02:23:21|instagram|user_25|Look at this outfit!     |2268 |\n",
      "|2025-05-13 02:23:21|instagram|user_37|Throwback to last summer |2110 |\n",
      "|2025-05-13 02:23:21|instagram|user_43|So in love with this meal|2920 |\n",
      "|2025-05-13 02:23:21|instagram|user_10|Morning vibes            |80   |\n",
      "|2025-05-13 02:23:21|instagram|user_39|Besties forever          |346  |\n",
      "|2025-05-13 02:23:21|instagram|user_50|This place is magical    |1301 |\n",
      "|2025-05-13 02:23:21|instagram|user_21|Beach days are the best  |1600 |\n",
      "|2025-05-13 02:23:21|instagram|user_13|My heart is full         |755  |\n",
      "|2025-05-13 02:23:21|instagram|user_24|Throwback to last summer |4326 |\n",
      "+-------------------+---------+-------+-------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/home/jovyan/notebooks/datalake/social_logs/\")\n",
    "df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
