{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "### <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "**Primer Examen**\n",
    "\n",
    "**Fecha**: 14 de Marzo del 2025\n",
    "\n",
    "**Nombre del estudiante**: Marco Albanese\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/14 13:39:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQL-Exam-1-Marco-Albanese\") \\\n",
    "    .master(\"spark://cd68d43f7ac6:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Read the Data (10 points):**\n",
    "\n",
    "Load the `employees.csv` and `departments.csv` files into PySpark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- employee_info: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- department_name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from equipo_mcqueen.spark_utils import SparkUtils\n",
    "\n",
    "employees_schema = SparkUtils.generate_schema([(\"employee_id\", \"IntegerType\"), (\"employee_info\", \"StringType\")])\n",
    "departments_schema = SparkUtils.generate_schema([(\"department_id\", \"IntegerType\"), (\"department_name\", \"StringType\"), (\"location\", \"StringType\")])\n",
    "\n",
    "employees = spark.read.schema(employees_schema).option(\"header\", \"true\").csv(\"/home/jovyan/notebooks/data/employees.csv\")\n",
    "departments = spark.read.schema(departments_schema).option(\"header\", \"true\").csv(\"/home/jovyan/notebooks/data/departments.csv\")\n",
    "\n",
    "employees.printSchema()\n",
    "departments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Extract the employee info from the JSON column (20 points):**\n",
    "\n",
    "Extract the following the columns: name (string), department_id (integer), salary (double), and hire_date (date) from the employee_info column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+---------+----------+\n",
      "|employee_id|   name|department_id|   salary| hire_date|\n",
      "+-----------+-------+-------------+---------+----------+\n",
      "|          1|Caitlyn|          103|115959.78|2002-06-10|\n",
      "|          2| Rachel|          104|100820.16|2009-07-01|\n",
      "|          3| Carrie|          105|114421.44|1998-12-10|\n",
      "+-----------+-------+-------------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "\n",
    "employees = employees.withColumn(\"name\", get_json_object(employees.employee_info, \"$.name\"))\n",
    "employees = employees.withColumn(\"department_id\", get_json_object(employees.employee_info, \"$.department_id\"))\n",
    "employees = employees.withColumn(\"salary\", get_json_object(employees.employee_info, \"$.salary\"))\n",
    "employees = employees.withColumn(\"hire_date\", get_json_object(employees.employee_info, \"$.hire_date\"))\n",
    "\n",
    "employees = employees.drop(\"employee_info\")\n",
    "\n",
    "employees.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Join Operations (10 points):**\n",
    "\n",
    "Join the `employees` DataFrame with the `departments` DataFrame on `department_id` to enrich the employee data with department details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+---------+----------+-------------+-------------------+--------+\n",
      "|employee_id|   name|department_id|   salary| hire_date|department_id|    department_name|location|\n",
      "+-----------+-------+-------------+---------+----------+-------------+-------------------+--------+\n",
      "|          1|Caitlyn|          103|115959.78|2002-06-10|          103|Sales and Marketing| Chicago|\n",
      "|          2| Rachel|          104|100820.16|2009-07-01|          104|   Data Engineering| Zapopan|\n",
      "|          3| Carrie|          105|114421.44|1998-12-10|          105|       Data Science| Seattle|\n",
      "+-----------+-------+-------------+---------+----------+-------------+-------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees = employees.join(departments, employees.department_id == departments.department_id, \"inner\")\n",
    "employees.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Transformations (Using `when()`) (10 points):**\n",
    "\n",
    "- Add a new column `salary_category` to the enriched `employee` DataFrame:\n",
    "  - If `salary` is greater than or equal to **55000**, set `salary_category` to \"High\".\n",
    "  - Otherwise, set `salary_category` to \"Low\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------------+---------+----------+-------------+-------------------+-------------+---------------+\n",
      "|employee_id|     name|department_id|   salary| hire_date|department_id|    department_name|     location|salary_category|\n",
      "+-----------+---------+-------------+---------+----------+-------------+-------------------+-------------+---------------+\n",
      "|          1|  Caitlyn|          103|115959.78|2002-06-10|          103|Sales and Marketing|      Chicago|           High|\n",
      "|          2|   Rachel|          104|100820.16|2009-07-01|          104|   Data Engineering|      Zapopan|           High|\n",
      "|          3|   Carrie|          105|114421.44|1998-12-10|          105|       Data Science|      Seattle|           High|\n",
      "|          4|    Renee|          104| 54688.13|1995-03-17|          104|   Data Engineering|      Zapopan|            Low|\n",
      "|          5|Gabriella|          109|106267.03|1995-02-09|          109|   Customer Service|San Francisco|           High|\n",
      "+-----------+---------+-------------+---------+----------+-------------+-------------------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "employees = employees.withColumn(\"salary_category\", when(employees.salary >= 55000, \"High\").otherwise(\"Low\"))\n",
    "employees.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Filter and Group (20 points):**\n",
    "\n",
    "- Create two new data frames: one that filters employees with a “High” salary and another that filters employees with a “Low” salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+-------------------+---------------+\n",
      "|employee_id|   name|   salary|    department_name|salary_category|\n",
      "+-----------+-------+---------+-------------------+---------------+\n",
      "|          1|Caitlyn|115959.78|Sales and Marketing|           High|\n",
      "|          2| Rachel|100820.16|   Data Engineering|           High|\n",
      "|          3| Carrie|114421.44|       Data Science|           High|\n",
      "+-----------+-------+---------+-------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+--------+--------+--------------------+---------------+\n",
      "|employee_id|    name|  salary|     department_name|salary_category|\n",
      "+-----------+--------+--------+--------------------+---------------+\n",
      "|          4|   Renee|54688.13|    Data Engineering|            Low|\n",
      "|          7|Jonathan|39323.42|Finance and Accou...|            Low|\n",
      "|         13|    Lisa|36032.49|    Data Engineering|            Low|\n",
      "+-----------+--------+--------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_salaries = employees.select(\"employee_id\", \"name\", \"salary\", \"department_name\", \"salary_category\")\n",
    "\n",
    "employees_high_salaries = employees_salaries.filter(employees_salaries.salary_category == \"High\")\n",
    "employees_high_salaries.show(3)\n",
    "\n",
    "employees_low_salaries = employees_salaries.filter(employees_salaries.salary_category == \"Low\")\n",
    "employees_low_salaries.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the average salary **per department** for the two newly created data frames, which contain the salaries of employees categorized as “High” and “Low.”  Resulting data frame for this transformation should contain only **department_name** and **avg_salary** columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|     department_name|        avg_salary|\n",
      "+--------------------+------------------+\n",
      "|Corporate Strateg...|102741.38324414717|\n",
      "| Sales and Marketing|100839.65275449108|\n",
      "|    Data Engineering|101626.29492163012|\n",
      "+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+------------------+\n",
      "|     department_name|        avg_salary|\n",
      "+--------------------+------------------+\n",
      "|Corporate Strateg...|41590.741833333326|\n",
      "| Sales and Marketing| 41150.40277777778|\n",
      "|    Data Engineering| 41358.50794117647|\n",
      "+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "employees_high_salaries_avg = employees_high_salaries.groupBy(\"department_name\").agg(avg(\"salary\").alias(\"avg_salary\"))\n",
    "employees_high_salaries_avg.show(3)\n",
    "\n",
    "employees_low_salaries_avg = employees_low_salaries.groupBy(\"department_name\").agg(avg(\"salary\").alias(\"avg_salary\"))\n",
    "employees_low_salaries_avg.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Sort (10 points):**\n",
    "- Find the Top 5 employees with highest salaries from employees categorized as “**High**”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    name|  salary|\n",
      "+--------+--------+\n",
      "|Jennifer| 99968.9|\n",
      "|     Ana|99937.19|\n",
      "|  Thomas|99807.14|\n",
      "| Raymond|99793.82|\n",
      "|  Monica|99777.47|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_high_salaries = employees_high_salaries.select(\"name\", \"salary\")\n",
    "top_high_salaries = top_high_salaries.orderBy(top_high_salaries.salary.desc()).limit(5)\n",
    "top_high_salaries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the Top 5 employees with highest salaries from employees categorized as “**Low**”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| name|  salary|\n",
      "+-----+--------+\n",
      "|Linda|54993.53|\n",
      "|Tammy|54991.71|\n",
      "|Aaron|54989.45|\n",
      "|Craig| 54945.2|\n",
      "|Aaron| 54937.3|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_low_salaries = employees_low_salaries.select(\"name\", \"salary\")\n",
    "top_low_salaries = top_low_salaries.orderBy(top_low_salaries.salary.desc()).limit(5)\n",
    "top_low_salaries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Find the number of employees with more years in the company (15 points).**\n",
    "- Compute a new column with the years in company for each employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+----------------+\n",
      "|employee_id|   name| hire_date|employment_years|\n",
      "+-----------+-------+----------+----------------+\n",
      "|          1|Caitlyn|2002-06-10|              22|\n",
      "|          2| Rachel|2009-07-01|              15|\n",
      "|          3| Carrie|1998-12-10|              26|\n",
      "+-----------+-------+----------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, current_date, floor\n",
    "\n",
    "employment_years = employees.select(\"employee_id\", \"name\", \"hire_date\")\n",
    "\n",
    "employment_years = employment_years.withColumn(\"employment_years\", floor(datediff(current_date(), \"hire_date\") / 365))\n",
    "employment_years.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the list of employees with more years in company and **count** them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|employment_years|count|\n",
      "+----------------+-----+\n",
      "|              37|    2|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_employment_years = employment_years.groupBy(\"employment_years\").count()\n",
    "max_employment_years = max_employment_years.orderBy(max_employment_years.employment_years.desc()).limit(1)\n",
    "max_employment_years.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Display the  Spark Plan (5 points):**\n",
    "\n",
    "- Add to your Notebook an Screenshot of the DAG associated with one of the actions of your spark application. Your name should be visible and it should be the name of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"./DAG 68 Examen Marco Albanese.png\"> </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
