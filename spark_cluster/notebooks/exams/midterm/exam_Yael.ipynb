{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "### <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "**Primer Examen**\n",
    "\n",
    "**Fecha**: 14 de Marzo del 2025\n",
    "\n",
    "**Nombre del estudiante**:\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/14 13:37:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQL-Exam-1-Yael-Rodriguez\") \\\n",
    "    .master(\"spark://8b1848b8768b:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- employee_info: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from equipo.spark_utils import SparkUtils\n",
    "\n",
    "columns_info = [\n",
    "    (\"employee_id\", \"string\"), \n",
    "    (\"employee_info\", \"string\")\n",
    "]\n",
    "\n",
    "employee_schema = SparkUtils.generate_schema(columns_info)\n",
    "\n",
    "employee_df = spark.read \\\n",
    "                .schema(employee_schema) \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/jovyan/notebooks/data/exam_P2025_ESI3914B/employees.csv\")\n",
    "\n",
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- department_name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from equipo.spark_utils import SparkUtils\n",
    "\n",
    "columns_info = [\n",
    "    (\"department_id\", \"string\"), \n",
    "    (\"department_name\", \"string\"),\n",
    "    (\"location\", \"string\")\n",
    "]\n",
    "\n",
    "departments_schema = SparkUtils.generate_schema(columns_info)\n",
    "\n",
    "departments_df = spark.read \\\n",
    "                .schema(departments_schema) \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/jovyan/notebooks/data/exam_P2025_ESI3914B/departments.csv\")\n",
    "\n",
    "departments_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------+-------------+---------+----------+\n",
      "|employee_id|       employee_info|     name|department_id|   salary| hire_date|\n",
      "+-----------+--------------------+---------+-------------+---------+----------+\n",
      "|          1|{'name': 'Caitlyn...|  Caitlyn|          103|115959.78|2002-06-10|\n",
      "|          2|{'name': 'Rachel'...|   Rachel|          104|100820.16|2009-07-01|\n",
      "|          3|{'name': 'Carrie'...|   Carrie|          105|114421.44|1998-12-10|\n",
      "|          4|{'name': 'Renee',...|    Renee|          104| 54688.13|1995-03-17|\n",
      "|          5|{'name': 'Gabriel...|Gabriella|          109|106267.03|1995-02-09|\n",
      "+-----------+--------------------+---------+-------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- employee_info: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "employee_df = employee_df.withColumn(\"name\", get_json_object(employee_df.employee_info, '$.name')) \\\n",
    "    .withColumn(\"department_id\", get_json_object(employee_df.employee_info, '$.department_id')) \\\n",
    "    .withColumn(\"salary\", get_json_object(employee_df.employee_info, '$.salary')) \\\n",
    "    .withColumn(\"hire_date\", get_json_object(employee_df.employee_info, '$.hire_date')) \n",
    "employee_df = employee_df.withColumn(\"department_id\", employee_df.department_id.cast('integer')) \\\n",
    "    .withColumn(\"salary\", employee_df.salary.cast('double')) \\\n",
    "    .withColumn(\"hire_date\", employee_df.hire_date.cast('date'))\n",
    "employee_df.show(n=5)\n",
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------+-------------+---------+----------+---------------+\n",
      "|employee_id|       employee_info|     name|department_id|   salary| hire_date|salary_category|\n",
      "+-----------+--------------------+---------+-------------+---------+----------+---------------+\n",
      "|          1|{'name': 'Caitlyn...|  Caitlyn|          103|115959.78|2002-06-10|           High|\n",
      "|          2|{'name': 'Rachel'...|   Rachel|          104|100820.16|2009-07-01|           High|\n",
      "|          3|{'name': 'Carrie'...|   Carrie|          105|114421.44|1998-12-10|           High|\n",
      "|          4|{'name': 'Renee',...|    Renee|          104| 54688.13|1995-03-17|            Low|\n",
      "|          5|{'name': 'Gabriel...|Gabriella|          109|106267.03|1995-02-09|           High|\n",
      "+-----------+--------------------+---------+-------------+---------+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "employee_df = employee_df.withColumn('salary_category', when(employee_df['salary'] > 55000, \"High\").otherwise(\"Low\"))\n",
    "employee_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+-------------+--------+----------+---------------+\n",
      "|employee_id|       employee_info|    name|department_id|  salary| hire_date|salary_category|\n",
      "+-----------+--------------------+--------+-------------+--------+----------+---------------+\n",
      "|          4|{'name': 'Renee',...|   Renee|          104|54688.13|1995-03-17|            Low|\n",
      "|          7|{'name': 'Jonatha...|Jonathan|          102|39323.42|2012-06-30|            Low|\n",
      "|         13|{'name': 'Lisa', ...|    Lisa|          104|36032.49|2019-05-16|            Low|\n",
      "+-----------+--------------------+--------+-------------+--------+----------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+--------------------+-------+-------------+---------+----------+---------------+\n",
      "|employee_id|       employee_info|   name|department_id|   salary| hire_date|salary_category|\n",
      "+-----------+--------------------+-------+-------------+---------+----------+---------------+\n",
      "|          1|{'name': 'Caitlyn...|Caitlyn|          103|115959.78|2002-06-10|           High|\n",
      "|          2|{'name': 'Rachel'...| Rachel|          104|100820.16|2009-07-01|           High|\n",
      "|          3|{'name': 'Carrie'...| Carrie|          105|114421.44|1998-12-10|           High|\n",
      "+-----------+--------------------+-------+-------------+---------+----------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "low_df = employee_df.filter(employee_df[\"salary_category\"] == 'Low')\n",
    "high_df = employee_df.filter(employee_df[\"salary_category\"] == 'High')\n",
    "\n",
    "low_df.show(n=3)\n",
    "high_df.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|department_id|       avg_salary|\n",
      "+-------------+-----------------+\n",
      "|          108|41426.43521126761|\n",
      "|          101|41751.64784810126|\n",
      "|          103|41150.40277777778|\n",
      "+-------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------+------------------+\n",
      "|department_id|        avg_salary|\n",
      "+-------------+------------------+\n",
      "|          108|  98714.3003086419|\n",
      "|          101|104999.43191489363|\n",
      "|          103|100839.65275449108|\n",
      "+-------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_low_df = low_df.select('salary', 'department_id' )\n",
    "grouped_low_df = grouped_low_df.groupBy('department_id').avg('salary').withColumnRenamed('avg(salary)', 'avg_salary')\n",
    "\n",
    "grouped_high_df = high_df.select('salary', 'department_id' )\n",
    "grouped_high_df = grouped_high_df.groupBy('department_id').avg('salary').withColumnRenamed('avg(salary)', 'avg_salary')\n",
    "\n",
    "grouped_low_df.show(n=3)\n",
    "grouped_high_df.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------+\n",
      "|employee_id| name|  salary|\n",
      "+-----------+-----+--------+\n",
      "|       3472|Linda|54993.53|\n",
      "|       2545|Tammy|54991.71|\n",
      "|        382|Aaron|54989.45|\n",
      "|       2153|Craig| 54945.2|\n",
      "|       3024|Aaron| 54937.3|\n",
      "+-----------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+\n",
      "|employee_id|     name|   salary|\n",
      "+-----------+---------+---------+\n",
      "|       1778|Gabriella|149989.73|\n",
      "|       3621|Katherine| 149979.3|\n",
      "|        346|     Ryan| 149963.1|\n",
      "|       3807|  Caitlyn|149956.54|\n",
      "|       3050|     Mark|149915.56|\n",
      "+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "low_df.createOrReplaceTempView(\"low\")\n",
    "high_df.createOrReplaceTempView(\"high\")\n",
    "\n",
    "spark.sql(\"SELECT employee_id, name, salary FROM low ORDER BY salary DESC \").show(5)\n",
    "spark.sql(\"SELECT employee_id, name, salary FROM high ORDER BY salary DESC \").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----+-------------+---------+----------+---------------+-----+\n",
      "|employee_id|       employee_info| name|department_id|   salary| hire_date|salary_category|years|\n",
      "+-----------+--------------------+-----+-------------+---------+----------+---------------+-----+\n",
      "|         59|{'name': 'Ana', '...|  Ana|          104| 85736.81|1988-12-31|           High|   37|\n",
      "|        103|{'name': 'Seth', ...| Seth|          101|106867.83|1988-08-29|           High|   37|\n",
      "|        120|{'name': 'Megan',...|Megan|          107| 56472.42|1988-04-03|           High|   37|\n",
      "|        159|{'name': 'Sarah',...|Sarah|          103| 53202.89|1988-04-14|            Low|   37|\n",
      "|        182|{'name': 'Mark', ...| Mark|          103|122033.27|1988-11-20|           High|   37|\n",
      "+-----------+--------------------+-----+-------------+---------+----------+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total = 88\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "employee_df = employee_df.withColumn('years', 2025 - year(employee_df['hire_date'])).orderBy(['years'], ascending = [False])\n",
    "years = employee_df.select('years').collect()[0]\n",
    "\n",
    "most_years = employee_df.filter(employee_df['years'] == years.years)\n",
    "most_years.show(n=5)\n",
    "\n",
    "print(\"Total =\", most_years.count())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
