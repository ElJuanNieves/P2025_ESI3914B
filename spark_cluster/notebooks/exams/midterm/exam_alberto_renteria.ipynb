{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "### <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "**Primer Examen**\n",
    "\n",
    "**Fecha**: 14 de Marzo del 2025\n",
    "\n",
    "**Nombre del estudiante**: Alberto Renteria Camacho\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQL-Exam-1-ALBERTO-RENTERIA\") \\\n",
    "    .master(\"spark://70f60bba1584:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Data (10 points):\n",
    "    Load the employees.csv and departments.csv files into PySpark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- employee_info: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from team_name.spark_utils import SparkUtils\n",
    "\n",
    "employees_schema = SparkUtils.generate_schema([(\"employee_id\", \"integer\"), (\"employee_info\", \"string\")])\n",
    "\n",
    "employees_df = spark.read \\\n",
    "                .schema(employees_schema) \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/jovyan/notebooks/data/employees.csv\")\n",
    "\n",
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- department_name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from team_name.spark_utils import SparkUtils\n",
    "\n",
    "departments_schema = SparkUtils.generate_schema([(\"department_id\", \"integer\"), (\"department_name\", \"string\"), (\"location\", \"string\")])\n",
    "\n",
    "departments_df = spark.read \\\n",
    "                .schema(departments_schema) \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/jovyan/notebooks/data/departments.csv\")\n",
    "\n",
    "departments_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the employee info from the JSON column (20 points):\n",
    "    Extract the following the columns: name (string), department_id (integer), salary (double), and hire_date (date) from the employee_info column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import get_json_object, col\n",
    "from pyspark.sql.types import IntegerType, DoubleType, DateType\n",
    "\n",
    "employees_df = employees_df.withColumn(\"name\", get_json_object(col(\"employee_info\"), \"$.name\")) \\\n",
    "    .withColumn(\"department_id\", get_json_object(col(\"employee_info\"), \"$.department_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"salary\", get_json_object(col(\"employee_info\"), \"$.salary\").cast(DoubleType())) \\\n",
    "    .withColumn(\"hire_date\", get_json_object(col(\"employee_info\"), \"$.hire_date\").cast(DateType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join Operations (10 points):\n",
    "    Join the employees DataFrame with the departments DataFrame on department_id to enrich the employee data with department details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------+-------------+---------+----------+-------------+-------------------+-------------+\n",
      "|employee_id|       employee_info|     name|department_id|   salary| hire_date|department_id|    department_name|     location|\n",
      "+-----------+--------------------+---------+-------------+---------+----------+-------------+-------------------+-------------+\n",
      "|          1|{'name': 'Caitlyn...|  Caitlyn|          103|115959.78|2002-06-10|          103|Sales and Marketing|      Chicago|\n",
      "|          2|{'name': 'Rachel'...|   Rachel|          104|100820.16|2009-07-01|          104|   Data Engineering|      Zapopan|\n",
      "|          3|{'name': 'Carrie'...|   Carrie|          105|114421.44|1998-12-10|          105|       Data Science|      Seattle|\n",
      "|          4|{'name': 'Renee',...|    Renee|          104| 54688.13|1995-03-17|          104|   Data Engineering|      Zapopan|\n",
      "|          5|{'name': 'Gabriel...|Gabriella|          109|106267.03|1995-02-09|          109|   Customer Service|San Francisco|\n",
      "+-----------+--------------------+---------+-------------+---------+----------+-------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_info_df = employees_df.join(departments_df, employees_df[\"department_id\"] == departments_df[\"department_id\"], \"left\")\n",
    "employees_info_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations (Using when()) (10 points):\n",
    "Add a new column salary_category to the enriched employee DataFrame:\n",
    "- If salary is greater than or equal to 55000, set salary_category to \"High\".\n",
    "- Otherwise, set salary_category to \"Low\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------+-------------+---------+----------+-------------+-------------------+-------------+---------------+\n",
      "|employee_id|       employee_info|     name|department_id|   salary| hire_date|department_id|    department_name|     location|salary_category|\n",
      "+-----------+--------------------+---------+-------------+---------+----------+-------------+-------------------+-------------+---------------+\n",
      "|          1|{'name': 'Caitlyn...|  Caitlyn|          103|115959.78|2002-06-10|          103|Sales and Marketing|      Chicago|           High|\n",
      "|          2|{'name': 'Rachel'...|   Rachel|          104|100820.16|2009-07-01|          104|   Data Engineering|      Zapopan|           High|\n",
      "|          3|{'name': 'Carrie'...|   Carrie|          105|114421.44|1998-12-10|          105|       Data Science|      Seattle|           High|\n",
      "|          4|{'name': 'Renee',...|    Renee|          104| 54688.13|1995-03-17|          104|   Data Engineering|      Zapopan|            Low|\n",
      "|          5|{'name': 'Gabriel...|Gabriella|          109|106267.03|1995-02-09|          109|   Customer Service|San Francisco|           High|\n",
      "+-----------+--------------------+---------+-------------+---------+----------+-------------+-------------------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "employees_info_df = employees_info_df.withColumn(\n",
    "    \"salary_category\",\n",
    "    when(employees_info_df[\"salary\"] >= 55000, \"High\").otherwise(\"Low\")\n",
    ")\n",
    "\n",
    "employees_info_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Filter and Group (20 points):\n",
    "- Create two new data frames: one that filters employees with a “High” salary and another that filters employees with a “Low” salary.\n",
    "- Calculate the average salary per department for the two newly created data frames, which contain the salaries of employees categorized as “High” and “Low.”  Resulting data frame for this transformation should contain only department_name and avg_salary columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, avg, max, min\n",
    "\n",
    "employees_high_df = employees_info_df.filter(employees_info_df[\"salary_category\"] == \"High\")\n",
    "employees_low_df = employees_info_df.filter(employees_info_df[\"salary_category\"] == \"Low\")\n",
    "\n",
    "high_avg = employees_high_df.select([\"department_name\", \"salary\"]).groupBy(\"department_name\").agg(avg(\"salary\").alias(\"avg_salary\"))\n",
    "low_avg = employees_low_df.select([\"department_name\", \"salary\"]).groupBy(\"department_name\").agg(avg(\"salary\").alias(\"avg_salary\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort (10 points): \n",
    "- Find the Top 5 employees with highest salaries from employees categorized as “High”\n",
    "- Find the Top 5 employees with highest salaries from employees categorized as “Low”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------+-------------+---------+----------+-------------+--------------------+-----------+---------------+\n",
      "|employee_id|       employee_info|     name|department_id|   salary| hire_date|department_id|     department_name|   location|salary_category|\n",
      "+-----------+--------------------+---------+-------------+---------+----------+-------------+--------------------+-----------+---------------+\n",
      "|       1778|{'name': 'Gabriel...|Gabriella|          101|149989.73|2018-09-14|          101|     Human Resources|  San Diego|           High|\n",
      "|       3621|{'name': 'Katheri...|Katherine|          101| 149979.3|2017-07-26|          101|     Human Resources|  San Diego|           High|\n",
      "|        346|{'name': 'Ryan', ...|     Ryan|          110| 149963.1|1990-07-03|          110|Corporate Strateg...|Los Angeles|           High|\n",
      "|       3807|{'name': 'Caitlyn...|  Caitlyn|          107|149956.54|2000-07-27|          107|               Legal|    Chicago|           High|\n",
      "|       3050|{'name': 'Mark', ...|     Mark|          107|149915.56|2007-11-06|          107|               Legal|    Chicago|           High|\n",
      "+-----------+--------------------+---------+-------------+---------+----------+-------------+--------------------+-----------+---------------+\n",
      "\n",
      "+-----------+--------------------+-----+-------------+--------+----------+-------------+--------------------+-------------+---------------+\n",
      "|employee_id|       employee_info| name|department_id|  salary| hire_date|department_id|     department_name|     location|salary_category|\n",
      "+-----------+--------------------+-----+-------------+--------+----------+-------------+--------------------+-------------+---------------+\n",
      "|       3472|{'name': 'Linda',...|Linda|          110|54993.53|2017-01-15|          110|Corporate Strateg...|  Los Angeles|            Low|\n",
      "|       2545|{'name': 'Tammy',...|Tammy|          104|54991.71|2004-12-07|          104|    Data Engineering|      Zapopan|            Low|\n",
      "|        382|{'name': 'Aaron',...|Aaron|          102|54989.45|2011-03-20|          102|Finance and Accou...|     New York|            Low|\n",
      "|       2153|{'name': 'Craig',...|Craig|          101| 54945.2|2016-04-24|          101|     Human Resources|    San Diego|            Low|\n",
      "|       3024|{'name': 'Aaron',...|Aaron|          109| 54937.3|1994-06-25|          109|    Customer Service|San Francisco|            Low|\n",
      "+-----------+--------------------+-----+-------------+--------+----------+-------------+--------------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "high_salary = employees_high_df.orderBy(employees_high_df[\"salary\"].desc()).limit(5)\n",
    "low_salary = employees_low_df.orderBy(employees_high_df[\"salary\"].desc()).limit(5)\n",
    "\n",
    "high_salary.show()\n",
    "low_salary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of employees with more years in the company (15 points).\n",
    "- Compute a new column with the years in company for each employee\n",
    "- Find the list of employees with more years in company and count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_ITERABLE] Column is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datediff, current_date\n\u001b[1;32m      3\u001b[0m employees_info_df \u001b[38;5;241m=\u001b[39m employees_info_df\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myears_in_company\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     (datediff(current_date(), employees_info_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhire_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m365\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(IntegerType())\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m max_years_df \u001b[38;5;241m=\u001b[39m employees_info_df\u001b[38;5;241m.\u001b[39magg(\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43memployees_info_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myears_in_company\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_years\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     10\u001b[0m max_years_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/pyspark/sql/column.py:718\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    719\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_ITERABLE\u001b[39m\u001b[38;5;124m\"\u001b[39m, message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjectName\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    720\u001b[0m     )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_ITERABLE] Column is not iterable."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, current_date\n",
    "\n",
    "employees_info_df = employees_info_df.withColumn(\n",
    "    \"years_in_company\",\n",
    "    (datediff(current_date(), employees_info_df[\"hire_date\"]) / 365).cast(IntegerType())\n",
    ")\n",
    "\n",
    "max_years_df = employees_info_df.select([\"years_in_company\"]).groupBy(\"years_in_company\").max(\"years_in_company\")\n",
    "\n",
    "max_years_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Display the  Spark Plan (5 points):\n",
    "- Add to your Notebook an Screenshot of the DAG associated with one of the actions of your spark application. Your name should be visible and it should be the name of the application.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
