{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Carrera: Ingeniería en Sistemas Computacionales** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "\n",
    "**Lab 10**: Heart attack prediction with Logistic Regression\n",
    "\n",
    "**Fecha**: 25/04/25\n",
    "\n",
    "**Nombre del Estudiante**: Angel Ramirez, Roberto Osorno, Yochabel Cazares, Samuel Romero\n",
    "\n",
    "**Profesor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLSpark-Logistic-Regression\") \\\n",
    "    .master(\"spark://f04d2745dc57:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- male: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- education: integer (nullable = true)\n",
      " |-- currentSmoker: integer (nullable = true)\n",
      " |-- cigsPerDay: integer (nullable = true)\n",
      " |-- BPMeds: integer (nullable = true)\n",
      " |-- prevalentStroke: integer (nullable = true)\n",
      " |-- prevalentHyp: integer (nullable = true)\n",
      " |-- diabetes: integer (nullable = true)\n",
      " |-- totChol: integer (nullable = true)\n",
      " |-- sysBP: double (nullable = true)\n",
      " |-- diaBP: double (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- heartRate: integer (nullable = true)\n",
      " |-- glucose: integer (nullable = true)\n",
      " |-- TenYearCHD: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---------+-------------+----------+------+---------------+------------+--------+-------+-----+-----+-----+---------+-------+----------+\n",
      "|male|age|education|currentSmoker|cigsPerDay|BPMeds|prevalentStroke|prevalentHyp|diabetes|totChol|sysBP|diaBP|BMI  |heartRate|glucose|TenYearCHD|\n",
      "+----+---+---------+-------------+----------+------+---------------+------------+--------+-------+-----+-----+-----+---------+-------+----------+\n",
      "|1   |39 |4        |0            |0         |0     |0              |0           |0       |195    |106.0|70.0 |26.97|80       |77     |0         |\n",
      "|0   |46 |2        |0            |0         |0     |0              |0           |0       |250    |121.0|81.0 |28.73|95       |76     |0         |\n",
      "|1   |48 |1        |1            |20        |0     |0              |0           |0       |245    |127.5|80.0 |25.34|75       |70     |0         |\n",
      "|0   |61 |3        |1            |30        |0     |0              |1           |0       |225    |150.0|95.0 |28.58|65       |103    |1         |\n",
      "|0   |46 |3        |1            |23        |0     |0              |0           |0       |285    |130.0|84.0 |23.1 |85       |85     |0         |\n",
      "+----+---+---------+-------------+----------+------+---------------+------------+--------+-------+-----+-----+-----+---------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from team_name.spark_utils import SparkUtils\n",
    "\n",
    "#Cargar el dataset\n",
    "heart_schema = SparkUtils.generate_schema([\n",
    "    (\"male\", \"integer\"), (\"age\", \"integer\"), (\"education\", \"integer\"),\n",
    "    (\"currentSmoker\", \"integer\"), (\"cigsPerDay\", \"integer\"),\n",
    "    (\"BPMeds\", \"integer\"), (\"prevalentStroke\", \"integer\"),\n",
    "    (\"prevalentHyp\", \"integer\"), (\"diabetes\", \"integer\"),\n",
    "    (\"totChol\", \"integer\"), (\"sysBP\", \"double\"), (\"diaBP\", \"double\"),\n",
    "    (\"BMI\", \"double\"), (\"heartRate\", \"integer\"), (\"glucose\", \"integer\"),\n",
    "    (\"TenYearCHD\", \"integer\")\n",
    "])\n",
    "\n",
    "heart_df = spark.read \\\n",
    "                .schema(heart_schema) \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/jovyan/notebooks/data/framingham.csv\")\n",
    "\n",
    "heart_df.printSchema()\n",
    "heart_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar valores nulos\n",
    "heart_df_clean = heart_df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensamblar las características en una sola columna vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "input_features = [\n",
    "    \"male\", \"age\", \"education\", \"currentSmoker\", \"cigsPerDay\",\n",
    "    \"BPMeds\", \"prevalentStroke\", \"prevalentHyp\", \"diabetes\",\n",
    "    \"totChol\", \"sysBP\", \"diaBP\", \"BMI\", \"heartRate\", \"glucose\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")\n",
    "data_with_features = assembler.transform(heart_df_clean).withColumnRenamed(\"TenYearCHD\", \"label\").select(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir los datos en conjuntos de entrenamiento y prueba: 80 % de datos de entrenamiento y 20 % de datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_with_features.randomSplit([0.8, 0.2], seed=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|[1.0,39.0,4.0,0.0...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|[1.0,48.0,1.0,1.0...|\n",
      "|    1|[0.0,61.0,3.0,1.0...|\n",
      "|    0|[0.0,46.0,3.0,1.0...|\n",
      "|    0|[0.0,43.0,2.0,0.0...|\n",
      "|    1|(15,[1,2,9,10,11,...|\n",
      "|    0|[0.0,45.0,2.0,1.0...|\n",
      "|    0|[1.0,52.0,1.0,0.0...|\n",
      "|    0|[1.0,43.0,1.0,1.0...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|[1.0,46.0,1.0,1.0...|\n",
      "|    0|[0.0,41.0,3.0,0.0...|\n",
      "|    1|[0.0,38.0,2.0,1.0...|\n",
      "|    0|[1.0,48.0,3.0,1.0...|\n",
      "|    1|[0.0,46.0,2.0,1.0...|\n",
      "|    0|[0.0,38.0,2.0,1.0...|\n",
      "|    0|[1.0,41.0,2.0,0.0...|\n",
      "|    0|[0.0,42.0,2.0,1.0...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "|    0|(15,[1,2,9,10,11,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Mostrar el dataset completo\n",
    "print(\"Original Dataset\")\n",
    "data_with_features.show()\n",
    "\n",
    "# Print train dataset\n",
    "print(\"train set\")\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el modelo de regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/25 14:50:57 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/04/25 14:50:57 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.5607363884781358,0.058016343645115036,-0.03862329485392394,0.08452119260032259,0.013677748121026475,0.2078776671534676,0.6011410129855924,0.23123946639244108,0.1554414037106486,0.0018853859790427148,0.01344076320004709,0.0004131080816762553,0.005988276385655795,-0.001638424476252627,0.007162714925878737]\n"
     ]
    }
   ],
   "source": [
    "#Imprimir los coeficientes\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "\n",
    "#Resumen del modelo de visualización\n",
    "training_summary = lr_model.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|            features|prediction|         probability|\n",
      "+--------------------+----------+--------------------+\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97710931709629...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97113718912072...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96821335312546...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.98006272989298...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.95368213125195...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96837933386689...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97360711539862...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96998684828337...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.95453168452457...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96422850127588...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97588475774837...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.95084460829496...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96009768991907...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96503935477800...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97349848460212...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.95055334242956...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97105485783000...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97207917474975...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96986265526016...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96098444320522...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Utilizando el modelo entrenado para hacer predicciones sobre los datos de prueba\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "#Mostrar las predicciones\n",
    "predictions.select(\"features\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8299866131191432\n",
      "Precision: 0.8038821954484605\n",
      "Recall: 0.8299866131191433\n",
      "F1 Score: 0.7697118793492835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
