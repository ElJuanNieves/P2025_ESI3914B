{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **Ejemplos de Aprendizaje Automático (Machine Learning): Logistic Regression** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de la conexión con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/26 15:39:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLSpark-Logistic-Regression\") \\\n",
    "    .master(\"spark://e3b046ba856a:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+-------------+----------+------+---------------+------------+--------+-------+-----+-----+-----+---------+-------+----------+\n",
      "|male| age|education|currentSmoker|cigsPerDay|BPMeds|prevalentStroke|prevalentHyp|diabetes|totChol|sysBP|diaBP|  BMI|heartRate|glucose|TenYearCHD|\n",
      "+----+----+---------+-------------+----------+------+---------------+------------+--------+-------+-----+-----+-----+---------+-------+----------+\n",
      "| 1.0|39.0|      4.0|          0.0|       0.0|   0.0|            0.0|         0.0|     0.0|  195.0|106.0| 70.0|26.97|     80.0|   77.0|       0.0|\n",
      "| 0.0|46.0|      2.0|          0.0|       0.0|   0.0|            0.0|         0.0|     0.0|  250.0|121.0| 81.0|28.73|     95.0|   76.0|       0.0|\n",
      "| 1.0|48.0|      1.0|          1.0|      20.0|   0.0|            0.0|         0.0|     0.0|  245.0|127.5| 80.0|25.34|     75.0|   70.0|       0.0|\n",
      "| 0.0|61.0|      3.0|          1.0|      30.0|   0.0|            0.0|         1.0|     0.0|  225.0|150.0| 95.0|28.58|     65.0|  103.0|       1.0|\n",
      "| 0.0|46.0|      3.0|          1.0|      23.0|   0.0|            0.0|         0.0|     0.0|  285.0|130.0| 84.0| 23.1|     85.0|   85.0|       0.0|\n",
      "| 0.0|43.0|      2.0|          0.0|       0.0|   0.0|            0.0|         1.0|     0.0|  228.0|180.0|110.0| 30.3|     77.0|   99.0|       0.0|\n",
      "| 0.0|63.0|      1.0|          0.0|       0.0|   0.0|            0.0|         0.0|     0.0|  205.0|138.0| 71.0|33.11|     60.0|   85.0|       1.0|\n",
      "| 0.0|45.0|      2.0|          1.0|      20.0|   0.0|            0.0|         0.0|     0.0|  313.0|100.0| 71.0|21.68|     79.0|   78.0|       0.0|\n",
      "| 1.0|52.0|      1.0|          0.0|       0.0|   0.0|            0.0|         1.0|     0.0|  260.0|141.5| 89.0|26.36|     76.0|   79.0|       0.0|\n",
      "| 1.0|43.0|      1.0|          1.0|      30.0|   0.0|            0.0|         1.0|     0.0|  225.0|162.0|107.0|23.61|     93.0|   88.0|       0.0|\n",
      "| 0.0|50.0|      1.0|          0.0|       0.0|   0.0|            0.0|         0.0|     0.0|  254.0|133.0| 76.0|22.91|     75.0|   76.0|       0.0|\n",
      "| 0.0|43.0|      2.0|          0.0|       0.0|   0.0|            0.0|         0.0|     0.0|  247.0|131.0| 88.0|27.64|     72.0|   61.0|       0.0|\n",
      "| 1.0|46.0|      1.0|          1.0|      15.0|   0.0|            0.0|         1.0|     0.0|  294.0|142.0| 94.0|26.31|     98.0|   64.0|       0.0|\n",
      "| 0.0|41.0|      3.0|          0.0|       0.0|   1.0|            0.0|         1.0|     0.0|  332.0|124.0| 88.0|31.31|     65.0|   84.0|       0.0|\n",
      "| 0.0|39.0|      2.0|          1.0|       9.0|   0.0|            0.0|         0.0|     0.0|  226.0|114.0| 64.0|22.35|     85.0|   NULL|       0.0|\n",
      "| 0.0|38.0|      2.0|          1.0|      20.0|   0.0|            0.0|         1.0|     0.0|  221.0|140.0| 90.0|21.35|     95.0|   70.0|       1.0|\n",
      "| 1.0|48.0|      3.0|          1.0|      10.0|   0.0|            0.0|         1.0|     0.0|  232.0|138.0| 90.0|22.37|     64.0|   72.0|       0.0|\n",
      "| 0.0|46.0|      2.0|          1.0|      20.0|   0.0|            0.0|         0.0|     0.0|  291.0|112.0| 78.0|23.38|     80.0|   89.0|       1.0|\n",
      "| 0.0|38.0|      2.0|          1.0|       5.0|   0.0|            0.0|         0.0|     0.0|  195.0|122.0| 84.5|23.24|     75.0|   78.0|       0.0|\n",
      "| 1.0|41.0|      2.0|          0.0|       0.0|   0.0|            0.0|         0.0|     0.0|  195.0|139.0| 88.0|26.88|     85.0|   65.0|       0.0|\n",
      "+----+----+---------+-------------+----------+------+---------------+------------+--------+-------+-----+-----+-----+---------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from team_name.spark_utils import SparkUtils\n",
    "\n",
    "csv_path = \"/home/jovyan/notebooks/data/heartDisease/framingham.csv\"\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = SparkUtils.generate_schema([\n",
    "    (\"male\", \"float\"),\n",
    "    (\"age\", \"float\"),\n",
    "    (\"education\", \"float\"),\n",
    "    (\"currentSmoker\", \"float\"),\n",
    "    (\"cigsPerDay\", \"float\"),\n",
    "    (\"BPMeds\", \"float\"),\n",
    "    (\"prevalentStroke\", \"float\"),\n",
    "    (\"prevalentHyp\", \"float\"),\n",
    "    (\"diabetes\", \"float\"),\n",
    "    (\"totChol\", \"float\"),\n",
    "    (\"sysBP\", \"float\"),\n",
    "    (\"diaBP\", \"float\"),\n",
    "    (\"BMI\", \"float\"),\n",
    "    (\"heartRate\", \"float\"),\n",
    "    (\"glucose\", \"float\"),\n",
    "    (\"TenYearCHD\", \"float\")\n",
    "])\n",
    "\n",
    "\n",
    "# Convert list to a DataFrame\n",
    "df = spark.read.csv(csv_path, header=True, schema=schema)\n",
    "#df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble the features into a single vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"feature_x1\", \"feature_x2\"], outputCol=\"features\")\n",
    "data_with_features = assembler.transform(df).select(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets 80% training data and 20% testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_with_features.randomSplit([0.8, 0.2], seed=57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset\n",
      "+-----+---------+\n",
      "|label| features|\n",
      "+-----+---------+\n",
      "|  1.0|[2.0,3.0]|\n",
      "|  0.0|[1.0,2.5]|\n",
      "|  1.0|[3.0,5.0]|\n",
      "|  0.0|[0.5,1.0]|\n",
      "|  1.0|[4.0,6.0]|\n",
      "+-----+---------+\n",
      "\n",
      "train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|label| features|\n",
      "+-----+---------+\n",
      "|  0.0|[1.0,2.5]|\n",
      "|  1.0|[2.0,3.0]|\n",
      "|  0.0|[0.5,1.0]|\n",
      "|  1.0|[4.0,6.0]|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Dataset\")\n",
    "data_with_features.show()\n",
    "\n",
    "# Print train dataset\n",
    "print(\"train set\")\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/25 14:32:17 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/04/25 14:32:17 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [2.346116998875653,0.7963873036415706]\n"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "\n",
    "# Display model summary\n",
    "training_summary = lr_model.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+\n",
      "| features|prediction|         probability|\n",
      "+---------+----------+--------------------+\n",
      "|[3.0,5.0]|       1.0|[0.00524886113385...|\n",
      "+---------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to make predictions on the test data\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"features\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"f1:{f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
