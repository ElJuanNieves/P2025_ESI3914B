{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electr칩nica, Sistemas e Inform치tica** </center>\n",
    "---\n",
    "## <center> **Carrera: Ing. en Sistemas Computacionales** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "\n",
    "**Lab 10**: Heart attack prediction with Logistic Regression\n",
    "\n",
    "**Fecha**: 11 de mayo del 2025\n",
    "\n",
    "**Nombre del Estudiante**: Marco Albanese, Vicente Siloe\n",
    "\n",
    "**Profesor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de la conexi칩n con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/07 07:47:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLSpark-Logistic-Regression\") \\\n",
    "    .master(\"spark://2da3617855ce:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparaci칩n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from equipo_mcqueen.spark_utils import SparkUtils\n",
    "\n",
    "heart_attack_data = [\n",
    "    (\"male\", \"IntegerType\"), \n",
    "    (\"age\", \"IntegerType\"), \n",
    "    (\"education\", \"IntegerType\"), \n",
    "    (\"currentSmoker\", \"IntegerType\"), \n",
    "    (\"cigsPerDay\", \"IntegerType\"), \n",
    "    (\"BPMeds\", \"IntegerType\"), \n",
    "    (\"prevalentStroke\", \"IntegerType\"), \n",
    "    (\"prevalentHyp\", \"IntegerType\"), \n",
    "    (\"diabetes\", \"IntegerType\"), \n",
    "    (\"totChol\", \"IntegerType\"), \n",
    "    (\"sysBP\", \"FloatType\"), \n",
    "    (\"diaBP\", \"FloatType\"), \n",
    "    (\"BMI\", \"FloatType\"), \n",
    "    (\"heartRate\", \"IntegerType\"), \n",
    "    (\"glucose\", \"IntegerType\"), \n",
    "    (\"TenYearCHD\", \"IntegerType\")\n",
    "]\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = SparkUtils.generate_schema(heart_attack_data)\n",
    "\n",
    "# Convert list to a DataFrame\n",
    "df = spark.read.schema(schema).option(\"header\", \"true\").csv(\"/home/jovyan/notebooks/data/framingham.csv\")\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the features into a single vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "heart_attack_cols = [\n",
    "    'male', 'age', 'education', 'currentSmoker', 'cigsPerDay', \n",
    "    'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes', \n",
    "    'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose'\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=heart_attack_cols, outputCol=\"features\")\n",
    "data_with_features = assembler.transform(df).select(\"TenYearCHD\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training and test sets 80% training data and 20% testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_with_features.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 07:48:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|TenYearCHD|            features|\n",
      "+----------+--------------------+\n",
      "|         0|[1.0,39.0,4.0,0.0...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|[1.0,48.0,1.0,1.0...|\n",
      "|         1|[0.0,61.0,3.0,1.0...|\n",
      "|         0|[0.0,46.0,3.0,1.0...|\n",
      "|         0|[0.0,43.0,2.0,0.0...|\n",
      "|         1|(15,[1,2,9,10,11,...|\n",
      "|         0|[0.0,45.0,2.0,1.0...|\n",
      "|         0|[1.0,52.0,1.0,0.0...|\n",
      "|         0|[1.0,43.0,1.0,1.0...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|[1.0,46.0,1.0,1.0...|\n",
      "|         0|[0.0,41.0,3.0,0.0...|\n",
      "|         1|[0.0,38.0,2.0,1.0...|\n",
      "|         0|[1.0,48.0,3.0,1.0...|\n",
      "|         1|[0.0,46.0,2.0,1.0...|\n",
      "|         0|[0.0,38.0,2.0,1.0...|\n",
      "|         0|[1.0,41.0,2.0,0.0...|\n",
      "|         0|[0.0,42.0,2.0,1.0...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "train set\n",
      "+----------+--------------------+\n",
      "|TenYearCHD|            features|\n",
      "+----------+--------------------+\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "|         0|(15,[1,2,9,10,11,...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Dataset\")\n",
    "data_with_features.show()\n",
    "\n",
    "# Print train dataset\n",
    "print(\"train set\")\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"TenYearCHD\", maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 07:49:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/07 07:49:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.5082844047138703,0.059286285190248735,-0.06811641200944737,0.20379021742850734,0.012573716113124526,0.4988565787897998,0.867662655614685,0.22752355015488057,-0.05674527489077539,0.0015813256796855003,0.011987921346163563,0.0008730866066968816,0.0035229175459439246,-0.0012276670059417456,0.007146752809847647]\n"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "\n",
    "# Display model summary\n",
    "training_summary = lr_model.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|            features|prediction|         probability|\n",
      "+--------------------+----------+--------------------+\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97777779601108...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97546664709744...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97702545674191...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97610701231714...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97624162471626...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97680162999968...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97099846501360...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96790675336005...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97452447483122...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97023384348738...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96037668903675...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97908644346531...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97328591682390...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.95937065727014...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97817335360252...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.97573661784566...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96443316279742...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96545014397557...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96955422964880...|\n",
      "|(15,[1,2,9,10,11,...|       0.0|[0.96852148891453...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to make predictions on the test data\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"features\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
