{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Carrera: <Coloca el nombre de tu carrera aqui>** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "\n",
    "**Lab 07**\n",
    "\n",
    "**Fecha**: 04/04/4045\n",
    "\n",
    "**Nombre del equipo**: Equipo 2.0\n",
    "\n",
    "**Profesor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLStructuredStreaming-Files\") \\\n",
    "    .master(\"spark://34ad4bc7c499:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Java Listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from equipo.log_listener import TrafficListener\n",
    "spark.streams.addListener(TrafficListener())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/04 17:13:00 ERROR MicroBatchExecution: Query [id = e4e5b7df-c42e-4f3e-917b-d16df8d1bdde, runId = fd6aa758-1315-4f86-9839-8399038a6691] terminated with error\n",
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:840)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1654)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n",
      "\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:366)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "25/04/04 17:13:00 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@28e401a[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@2c84fdfd[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@64bc45b]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@47d31f4[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "Exception in thread \"stream execution thread for [id = e4e5b7df-c42e-4f3e-917b-d16df8d1bdde, runId = fd6aa758-1315-4f86-9839-8399038a6691]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:426)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:360)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:340)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:100)\n",
      "\t... 11 more\n"
     ]
    }
   ],
   "source": [
    "log_lines = spark \\\n",
    "                .readStream \\\n",
    "                .format(\"text\") \\\n",
    "                .option(\"maxFilesPerTrigger\", 1) \\\n",
    "                .load(\"/home/jovyan/notebooks/data/log_streaming/input/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- logs_array: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- server: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "log_df = log_lines.select(split(log_lines.value, \" | \").alias(\"logs_array\"))\n",
    "log_df = log_df.withColumn(\"timestamp\", log_df[\"logs_array\"].getItem(0).cast(\"string\"))\n",
    "log_df = log_df.withColumn(\"level\", log_df[\"logs_array\"].getItem(1).cast(\"string\"))\n",
    "log_df = log_df.withColumn(\"message\", log_df[\"logs_array\"].getItem(2).cast(\"string\"))\n",
    "log_df = log_df.withColumn(\"server\", log_df[\"logs_array\"].getItem(3).cast(\"string\"))\n",
    "\n",
    "log_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- logs_array: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- server: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df = log_df.filter(log_df['level'] == \"ERROR\")\n",
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started: e4e5b7df-c42e-4f3e-917b-d16df8d1bdde\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/04 17:13:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 83\n",
      "-------------------------------------------\n",
      "+----------+---------+-----+-------+------+\n",
      "|logs_array|timestamp|level|message|server|\n",
      "+----------+---------+-----+-------+------+\n",
      "+----------+---------+-----+-------+------+\n",
      "\n",
      "Query made progress: {\n",
      "  \"id\" : \"e4e5b7df-c42e-4f3e-917b-d16df8d1bdde\",\n",
      "  \"runId\" : \"8c4d6383-1cde-410f-8af8-6b8933561437\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-04T17:13:05.327Z\",\n",
      "  \"batchId\" : 83,\n",
      "  \"numInputRows\" : 37,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 14.925373134328357,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 2313,\n",
      "    \"commitOffsets\" : 55,\n",
      "    \"getBatch\" : 61,\n",
      "    \"queryPlanning\" : 17,\n",
      "    \"triggerExecution\" : 2479\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/log_streaming/input]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 82\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 83\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 37,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 14.925373134328357\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@76a5e571\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/04 17:13:15 WARN FileStreamSource: Listed 291 file(s) in 5042 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 84\n",
      "-------------------------------------------\n",
      "+----------+---------+-----+-------+------+\n",
      "|logs_array|timestamp|level|message|server|\n",
      "+----------+---------+-----+-------+------+\n",
      "+----------+---------+-----+-------+------+\n",
      "\n",
      "Query made progress: {\n",
      "  \"id\" : \"e4e5b7df-c42e-4f3e-917b-d16df8d1bdde\",\n",
      "  \"runId\" : \"8c4d6383-1cde-410f-8af8-6b8933561437\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-04T17:13:10.000Z\",\n",
      "  \"batchId\" : 84,\n",
      "  \"numInputRows\" : 35,\n",
      "  \"inputRowsPerSecond\" : 7.4898352236250805,\n",
      "  \"processedRowsPerSecond\" : 6.271277548826375,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 283,\n",
      "    \"commitOffsets\" : 56,\n",
      "    \"getBatch\" : 73,\n",
      "    \"latestOffset\" : 5101,\n",
      "    \"queryPlanning\" : 11,\n",
      "    \"triggerExecution\" : 5581,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/log_streaming/input]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 83\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 84\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 35,\n",
      "    \"inputRowsPerSecond\" : 7.4898352236250805,\n",
      "    \"processedRowsPerSecond\" : 6.271277548826375\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@76a5e571\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n",
      "-------------------------------------------\n",
      "Batch: 85\n",
      "-------------------------------------------\n",
      "+----------+---------+-----+-------+------+\n",
      "|logs_array|timestamp|level|message|server|\n",
      "+----------+---------+-----+-------+------+\n",
      "+----------+---------+-----+-------+------+\n",
      "\n",
      "Query made progress: {\n",
      "  \"id\" : \"e4e5b7df-c42e-4f3e-917b-d16df8d1bdde\",\n",
      "  \"runId\" : \"8c4d6383-1cde-410f-8af8-6b8933561437\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-04T17:13:20.000Z\",\n",
      "  \"batchId\" : 85,\n",
      "  \"numInputRows\" : 83,\n",
      "  \"inputRowsPerSecond\" : 8.3,\n",
      "  \"processedRowsPerSecond\" : 174.36974789915968,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 245,\n",
      "    \"commitOffsets\" : 54,\n",
      "    \"getBatch\" : 54,\n",
      "    \"latestOffset\" : 55,\n",
      "    \"queryPlanning\" : 13,\n",
      "    \"triggerExecution\" : 476,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/log_streaming/input]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 84\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 85\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 83,\n",
      "    \"inputRowsPerSecond\" : 8.3,\n",
      "    \"processedRowsPerSecond\" : 174.36974789915968\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@76a5e571\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n",
      "ALERT: The volume of data if high (numInputRows >= 50)\n",
      "-------------------------------------------\n",
      "Batch: 86\n",
      "-------------------------------------------\n",
      "+----------+---------+-----+-------+------+\n",
      "|logs_array|timestamp|level|message|server|\n",
      "+----------+---------+-----+-------+------+\n",
      "+----------+---------+-----+-------+------+\n",
      "\n",
      "Query made progress: {\n",
      "  \"id\" : \"e4e5b7df-c42e-4f3e-917b-d16df8d1bdde\",\n",
      "  \"runId\" : \"8c4d6383-1cde-410f-8af8-6b8933561437\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-04T17:13:30.000Z\",\n",
      "  \"batchId\" : 86,\n",
      "  \"numInputRows\" : 77,\n",
      "  \"inputRowsPerSecond\" : 7.7,\n",
      "  \"processedRowsPerSecond\" : 177.41935483870967,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 211,\n",
      "    \"commitOffsets\" : 57,\n",
      "    \"getBatch\" : 52,\n",
      "    \"latestOffset\" : 53,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 434,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/log_streaming/input]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 85\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 86\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 77,\n",
      "    \"inputRowsPerSecond\" : 7.7,\n",
      "    \"processedRowsPerSecond\" : 177.41935483870967\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@76a5e571\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n",
      "ALERT: The volume of data if high (numInputRows >= 50)\n",
      "-------------------------------------------\n",
      "Batch: 87\n",
      "-------------------------------------------\n",
      "+----------+---------+-----+-------+------+\n",
      "|logs_array|timestamp|level|message|server|\n",
      "+----------+---------+-----+-------+------+\n",
      "+----------+---------+-----+-------+------+\n",
      "\n",
      "Query made progress: {\n",
      "  \"id\" : \"e4e5b7df-c42e-4f3e-917b-d16df8d1bdde\",\n",
      "  \"runId\" : \"8c4d6383-1cde-410f-8af8-6b8933561437\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-04T17:13:40.000Z\",\n",
      "  \"batchId\" : 87,\n",
      "  \"numInputRows\" : 69,\n",
      "  \"inputRowsPerSecond\" : 6.9,\n",
      "  \"processedRowsPerSecond\" : 159.72222222222223,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 206,\n",
      "    \"commitOffsets\" : 58,\n",
      "    \"getBatch\" : 54,\n",
      "    \"latestOffset\" : 53,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 432,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/log_streaming/input]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 86\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 87\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 69,\n",
      "    \"inputRowsPerSecond\" : 6.9,\n",
      "    \"processedRowsPerSecond\" : 159.72222222222223\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@76a5e571\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n",
      "ALERT: The volume of data if high (numInputRows >= 50)\n",
      "-------------------------------------------\n",
      "Batch: 88\n",
      "-------------------------------------------\n",
      "+----------+---------+-----+-------+------+\n",
      "|logs_array|timestamp|level|message|server|\n",
      "+----------+---------+-----+-------+------+\n",
      "+----------+---------+-----+-------+------+\n",
      "\n",
      "Query made progress: {\n",
      "  \"id\" : \"e4e5b7df-c42e-4f3e-917b-d16df8d1bdde\",\n",
      "  \"runId\" : \"8c4d6383-1cde-410f-8af8-6b8933561437\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-04T17:13:50.000Z\",\n",
      "  \"batchId\" : 88,\n",
      "  \"numInputRows\" : 85,\n",
      "  \"inputRowsPerSecond\" : 8.5,\n",
      "  \"processedRowsPerSecond\" : 214.1057934508816,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 177,\n",
      "    \"commitOffsets\" : 54,\n",
      "    \"getBatch\" : 56,\n",
      "    \"latestOffset\" : 51,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 397,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/log_streaming/input]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 87\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 88\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 85,\n",
      "    \"inputRowsPerSecond\" : 8.5,\n",
      "    \"processedRowsPerSecond\" : 214.1057934508816\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@76a5e571\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n",
      "ALERT: The volume of data if high (numInputRows >= 50)\n",
      "-------------------------------------------\n",
      "Batch: 89\n",
      "-------------------------------------------\n",
      "+----------+---------+-----+-------+------+\n",
      "|logs_array|timestamp|level|message|server|\n",
      "+----------+---------+-----+-------+------+\n",
      "+----------+---------+-----+-------+------+\n",
      "\n",
      "Query made progress: {\n",
      "  \"id\" : \"e4e5b7df-c42e-4f3e-917b-d16df8d1bdde\",\n",
      "  \"runId\" : \"8c4d6383-1cde-410f-8af8-6b8933561437\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-04T17:14:00.000Z\",\n",
      "  \"batchId\" : 89,\n",
      "  \"numInputRows\" : 88,\n",
      "  \"inputRowsPerSecond\" : 8.8,\n",
      "  \"processedRowsPerSecond\" : 201.8348623853211,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 181,\n",
      "    \"commitOffsets\" : 64,\n",
      "    \"getBatch\" : 60,\n",
      "    \"latestOffset\" : 63,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 436,\n",
      "    \"walCommit\" : 58\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/log_streaming/input]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 88\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 89\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 88,\n",
      "    \"inputRowsPerSecond\" : 8.8,\n",
      "    \"processedRowsPerSecond\" : 201.8348623853211\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@76a5e571\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n",
      "ALERT: The volume of data if high (numInputRows >= 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_files = log_df \\\n",
    "                .writeStream \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .format(\"console\") \\\n",
    "                .trigger(processingTime='10 seconds') \\\n",
    "                .option(\"path\", \"/home/jovyan/notebooks/data/log_streaming/output/\") \\\n",
    "                .option(\"checkpointLocation\", \"/home/jovyan/checkpoint\") \\\n",
    "                .start()\n",
    "query_files.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                  (0 + 1) / 1][Stage 10:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+-------+------+\n",
      "|logs_array|timestamp|level|message|server|\n",
      "+----------+---------+-----+-------+------+\n",
      "+----------+---------+-----+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "speed_df = spark \\\n",
    "            .read \\\n",
    "            .parquet(\"/home/jovyan/notebooks/data/log_streaming/output/\")\n",
    "\n",
    "speed_df.show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/04 17:14:11 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 90, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "25/04/04 17:14:11 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 90, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "25/04/04 17:14:11 ERROR MicroBatchExecution: Query [id = e4e5b7df-c42e-4f3e-917b-d16df8d1bdde, runId = 8c4d6383-1cde-410f-8af8-6b8933561437] terminated with error\n",
      "org.apache.spark.SparkException: Job 10 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\n",
      "\tat scala.collection.mutable.HashSet$Node.foreach(HashSet.scala:435)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:361)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
