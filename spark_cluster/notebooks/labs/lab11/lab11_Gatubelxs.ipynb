{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **Ejemplos de Aprendizaje Automático (Machine Learning): Decision Trees** </center>\n",
    "\n",
    "---\n",
    "**Equipo**:\n",
    "- Luis Raúl Acosta Mendoza\n",
    "- Samantha Abigail Quintero Valadez \n",
    "- Arturo Benjamin Vergara Romo\n",
    "\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de la conexión con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/30 01:41:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLSpark-Decision-Trees\") \\\n",
    "    .master(\"spark://be6296989c4d:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gatubelxs.spark_utils\n",
    "columns_info = [ (\"Id\", \"integer\"),\n",
    "                (\"SepalLenghtCm\", \"double\"),\n",
    "                (\"SepalWidthCm\", \"double\"),\n",
    "                (\"PetalLengthCm\", \"double\"),\n",
    "                (\"PetalWidthCm\", \"double\"),\n",
    "                (\"Species\", \"string\")]\n",
    "\n",
    "schema = gatubelxs.spark_utils.SparkUtils.generate_schema(columns_info)\n",
    "\n",
    "# Create DataFrame\n",
    "strokes_df = spark \\\n",
    "                .read \\\n",
    "                .schema(schema) \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/jovyan/notebooks/data/Iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble the features into a single vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n",
    "df_label = label_indexer.fit(strokes_df).transform(strokes_df)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"SepalLenghtCm\", \"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\"], outputCol=\"features\")\n",
    "data_with_features = assembler.transform(df_label).select(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets 80% training data and 20% testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_with_features.randomSplit([0.8, 0.2], seed=97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset\n",
      "+-----+-----------------+\n",
      "|label|         features|\n",
      "+-----+-----------------+\n",
      "|  0.0|[5.1,3.5,1.4,0.2]|\n",
      "|  0.0|[4.9,3.0,1.4,0.2]|\n",
      "|  0.0|[4.7,3.2,1.3,0.2]|\n",
      "|  0.0|[4.6,3.1,1.5,0.2]|\n",
      "|  0.0|[5.0,3.6,1.4,0.2]|\n",
      "|  0.0|[5.4,3.9,1.7,0.4]|\n",
      "|  0.0|[4.6,3.4,1.4,0.3]|\n",
      "|  0.0|[5.0,3.4,1.5,0.2]|\n",
      "|  0.0|[4.4,2.9,1.4,0.2]|\n",
      "|  0.0|[4.9,3.1,1.5,0.1]|\n",
      "|  0.0|[5.4,3.7,1.5,0.2]|\n",
      "|  0.0|[4.8,3.4,1.6,0.2]|\n",
      "|  0.0|[4.8,3.0,1.4,0.1]|\n",
      "|  0.0|[4.3,3.0,1.1,0.1]|\n",
      "|  0.0|[5.8,4.0,1.2,0.2]|\n",
      "|  0.0|[5.7,4.4,1.5,0.4]|\n",
      "|  0.0|[5.4,3.9,1.3,0.4]|\n",
      "|  0.0|[5.1,3.5,1.4,0.3]|\n",
      "|  0.0|[5.7,3.8,1.7,0.3]|\n",
      "|  0.0|[5.1,3.8,1.5,0.3]|\n",
      "+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "train set\n",
      "+-----+-----------------+\n",
      "|label|         features|\n",
      "+-----+-----------------+\n",
      "|  0.0|[4.3,3.0,1.1,0.1]|\n",
      "|  0.0|[4.4,2.9,1.4,0.2]|\n",
      "|  0.0|[4.4,3.0,1.3,0.2]|\n",
      "|  0.0|[4.4,3.2,1.3,0.2]|\n",
      "|  0.0|[4.6,3.1,1.5,0.2]|\n",
      "|  0.0|[4.6,3.2,1.4,0.2]|\n",
      "|  0.0|[4.6,3.4,1.4,0.3]|\n",
      "|  0.0|[4.6,3.6,1.0,0.2]|\n",
      "|  0.0|[4.7,3.2,1.6,0.2]|\n",
      "|  0.0|[4.8,3.0,1.4,0.1]|\n",
      "|  0.0|[4.8,3.0,1.4,0.3]|\n",
      "|  0.0|[4.8,3.1,1.6,0.2]|\n",
      "|  0.0|[4.8,3.4,1.6,0.2]|\n",
      "|  0.0|[4.8,3.4,1.9,0.2]|\n",
      "|  0.0|[4.9,3.0,1.4,0.2]|\n",
      "|  0.0|[4.9,3.1,1.5,0.1]|\n",
      "|  0.0|[4.9,3.1,1.5,0.1]|\n",
      "|  0.0|[5.0,3.0,1.6,0.2]|\n",
      "|  0.0|[5.0,3.2,1.2,0.2]|\n",
      "|  0.0|[5.0,3.4,1.5,0.2]|\n",
      "+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Dataset\")\n",
    "data_with_features.show()\n",
    "\n",
    "# Print train dataset\n",
    "print(\"train set\")\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Initialize and train the Decision Tree model\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model summary:DecisionTreeClassificationModel: uid=DecisionTreeClassifier_2a8cd2c64ddb, depth=5, numNodes=15, numClasses=3, numFeatures=4\n",
      "  If (feature 2 <= 2.5999999999999996)\n",
      "   Predict: 0.0\n",
      "  Else (feature 2 > 2.5999999999999996)\n",
      "   If (feature 2 <= 4.85)\n",
      "    If (feature 3 <= 1.65)\n",
      "     Predict: 1.0\n",
      "    Else (feature 3 > 1.65)\n",
      "     If (feature 1 <= 2.8499999999999996)\n",
      "      Predict: 2.0\n",
      "     Else (feature 1 > 2.8499999999999996)\n",
      "      Predict: 1.0\n",
      "   Else (feature 2 > 4.85)\n",
      "    If (feature 3 <= 1.75)\n",
      "     If (feature 0 <= 6.35)\n",
      "      Predict: 2.0\n",
      "     Else (feature 0 > 6.35)\n",
      "      If (feature 0 <= 6.95)\n",
      "       Predict: 1.0\n",
      "      Else (feature 0 > 6.95)\n",
      "       Predict: 2.0\n",
      "    Else (feature 3 > 1.75)\n",
      "     Predict: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_model = dt.fit(train_df)\n",
    "\n",
    "# Display model summary\n",
    "print(\"Decision Tree model summary:{0}\".format(dt_model.toDebugString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import OneVsRest\n",
    "\n",
    "# Initialize the LinearSVC classifier for binary\n",
    "# classification\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.01)\n",
    "# Set up OneVsRest classifier for multi-class\n",
    "# classification\n",
    "ovr = OneVsRest(classifier=lsvc)\n",
    "# Train the model\n",
    "ovr_model = ovr.fit(train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n",
      "|         features|prediction|\n",
      "+-----------------+----------+\n",
      "|[4.5,2.3,1.3,0.3]|       0.0|\n",
      "|[4.7,3.2,1.3,0.2]|       0.0|\n",
      "|[4.9,3.1,1.5,0.1]|       0.0|\n",
      "|[5.0,3.3,1.4,0.2]|       0.0|\n",
      "|[5.1,3.3,1.7,0.5]|       0.0|\n",
      "|[5.1,3.8,1.5,0.3]|       0.0|\n",
      "|[5.2,3.4,1.4,0.2]|       0.0|\n",
      "|[5.2,4.1,1.5,0.1]|       0.0|\n",
      "|[5.3,3.7,1.5,0.2]|       0.0|\n",
      "|[5.4,3.9,1.3,0.4]|       0.0|\n",
      "|[5.5,3.5,1.3,0.2]|       0.0|\n",
      "|[5.7,3.8,1.7,0.3]|       0.0|\n",
      "|[5.7,4.4,1.5,0.4]|       0.0|\n",
      "|[5.0,2.3,3.3,1.0]|       1.0|\n",
      "|[5.1,2.5,3.0,1.1]|       1.0|\n",
      "|[5.5,2.4,3.7,1.0]|       1.0|\n",
      "|[5.6,2.9,3.6,1.3]|       1.0|\n",
      "|[5.6,3.0,4.1,1.3]|       1.0|\n",
      "|[5.7,2.6,3.5,1.0]|       1.0|\n",
      "|[5.8,2.7,4.1,1.0]|       1.0|\n",
      "+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use the trained model to make predictions on the test data\n",
    "predictions = dt_model.transform(test_df)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"features\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (2.2.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ovr = ovr_model.transform(test_df)\n",
    "\n",
    "# Show predictions\n",
    "#predictions_ovr.select(\"features\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9117647058823529\n",
      "Precision: 0.9158496732026143\n",
      "Recall: 0.9117647058823528\n",
      "F1 Score: 0.9125951557093426\n"
     ]
    }
   ],
   "source": [
    "#Tree\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",\n",
    "                            predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, \n",
    "                  {evaluator.metricName: \"accuracy\"})\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "precision = evaluator.evaluate(predictions,\n",
    "                  {evaluator.metricName: \"weightedPrecision\"})\n",
    "print(f\"Precision: {precision}\")\n",
    "recall = evaluator.evaluate(predictions,\n",
    "                  {evaluator.metricName: \"weightedRecall\"})\n",
    "print(f\"Recall: {recall}\")\n",
    "f1 = evaluator.evaluate(predictions,\n",
    "                {evaluator.metricName: \"f1\"})\n",
    "print(f\"F1 Score: {f1}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 01:44:51 WARN TaskSetManager: Lost task 0.0 in stage 195.0 (TID 198) (172.18.0.3 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1377, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1419, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 708, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1421, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1205, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 32, in <module>\n",
      "    import numpy as np\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:259)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1293)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "25/04/30 01:44:52 ERROR TaskSetManager: Task 0 in stage 195.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n    for item in iterator:\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n    yield self._read_with_length(stream)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1377, in _parse_datatype_json_string\n    return _parse_datatype_json_value(json.loads(json_string))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1419, in _parse_datatype_json_value\n    return _all_complex_types[tpe].fromJson(json_value)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in fromJson\n    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in <listcomp>\n    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n                       ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 708, in fromJson\n    _parse_datatype_json_value(json[\"type\"]),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1421, in _parse_datatype_json_value\n    return UserDefinedType.fromJson(json_value)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1205, in fromJson\n    m = __import__(pyModule, globals(), locals(), [pyClass])\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n    from pyspark.ml.base import (\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/ml/base.py\", line 40, in <module>\n    from pyspark.ml.param import P\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 32, in <module>\n    import numpy as np\nModuleNotFoundError: No module named 'numpy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MulticlassClassificationEvaluator\n\u001b[1;32m      4\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m MulticlassClassificationEvaluator(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m                             predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions_ovr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m{\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetricName\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m precision \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(predictions_ovr,\n\u001b[1;32m     11\u001b[0m                   {evaluator\u001b[38;5;241m.\u001b[39mmetricName: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweightedPrecision\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/pyspark/ml/evaluation.py:109\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/pyspark/ml/evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n    for item in iterator:\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n    yield self._read_with_length(stream)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1377, in _parse_datatype_json_string\n    return _parse_datatype_json_value(json.loads(json_string))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1419, in _parse_datatype_json_value\n    return _all_complex_types[tpe].fromJson(json_value)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in fromJson\n    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in <listcomp>\n    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n                       ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 708, in fromJson\n    _parse_datatype_json_value(json[\"type\"]),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1421, in _parse_datatype_json_value\n    return UserDefinedType.fromJson(json_value)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1205, in fromJson\n    m = __import__(pyModule, globals(), locals(), [pyClass])\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n    from pyspark.ml.base import (\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/ml/base.py\", line 40, in <module>\n    from pyspark.ml.param import P\n  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 32, in <module>\n    import numpy as np\nModuleNotFoundError: No module named 'numpy'\n"
     ]
    }
   ],
   "source": [
    "#Tree\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",\n",
    "                            predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions_ovr, \n",
    "                  {evaluator.metricName: \"accuracy\"})\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "precision = evaluator.evaluate(predictions_ovr,\n",
    "                  {evaluator.metricName: \"weightedPrecision\"})\n",
    "print(f\"Precision: {precision}\")\n",
    "recall = evaluator.evaluate(predictions_ovr,\n",
    "                  {evaluator.metricName: \"weightedRecall\"})\n",
    "print(f\"Recall: {recall}\")\n",
    "f1 = evaluator.evaluate(predictions_ovr,\n",
    "                {evaluator.metricName: \"f1\"})\n",
    "print(f\"F1 Score: {f1}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
