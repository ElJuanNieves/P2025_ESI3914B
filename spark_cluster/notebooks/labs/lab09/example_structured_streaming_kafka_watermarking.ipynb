{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **Ejemplos de Spark: Structured Streaming (Kafka + Watermarking)** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de la conexión con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLStructuredStreaming-Kafka-Watermarking\") \\\n",
    "    .master(\"spark://be6296989c4d:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_lines = spark \\\n",
    "                .readStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"5a7ab902bd93:9093\") \\\n",
    "                .option(\"subscribe\", \"kafka-spark-example\") \\\n",
    "                .load()\n",
    "\n",
    "kafka_lines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform binary data into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = kafka_lines.withColumn(\"value_str\", kafka_lines.value.cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = false)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "words = kafka_df.select(explode(split(kafka_df.value, \" \")).alias(\"word\"), \"timestamp\")\n",
    "words.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando el mecanismo para manejar datos tardios con marcas de agua (watermarking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "windowed_counts =  words \\\n",
    "                        .withWatermark(\"timestamp\", \"2 minutes\") \\\n",
    "                        .groupBy(window(words.timestamp, \n",
    "                                        \"20 seconds\", # Window duration \n",
    "                                        \"10 seconds\"), # Slide duration\n",
    "                                 words.word) \\\n",
    "                        .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del \"Sink\" del stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 14:42:28 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f77e547a-ca16-4ce1-af2a-43acb1d0dece. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/04/08 14:42:28 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/04/08 14:42:28 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----+-----+\n",
      "|window|word|count|\n",
      "+------+----+-----+\n",
      "+------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 14:43:00 ERROR MicroBatchExecution: Query [id = 753d6490-35b3-4e03-acd7-ecec8daaaa88, runId = 2f3bcea9-df73-436c-a8d9-5621021993ce] terminated with error\n",
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:840)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1654)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.BaseStateStoreRDD.<init>(StateStoreRDD.scala:42)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.<init>(StateStoreRDD.scala:81)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.mapPartitionsWithReadStateStore(package.scala:106)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreRestoreExec.doExecute(statefulOperators.scala:427)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs(AggregateCodegenSupport.scala:89)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs$(AggregateCodegenSupport.scala:88)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.doExecute(statefulOperators.scala:485)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs(AggregateCodegenSupport.scala:89)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs$(AggregateCodegenSupport.scala:88)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:366)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "25/04/08 14:43:00 ERROR MicroBatchExecution: Query [id = a260ec59-37a1-4ada-bf0e-b3ae2ba797e9, runId = a8e7f450-6607-4300-9a01-a218be27302d] terminated with error\n",
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:840)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1654)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.BaseStateStoreRDD.<init>(StateStoreRDD.scala:42)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.<init>(StateStoreRDD.scala:81)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.mapPartitionsWithReadStateStore(package.scala:106)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreRestoreExec.doExecute(statefulOperators.scala:427)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs(AggregateCodegenSupport.scala:89)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs$(AggregateCodegenSupport.scala:88)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.doExecute(statefulOperators.scala:485)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs(AggregateCodegenSupport.scala:89)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs$(AggregateCodegenSupport.scala:88)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:366)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "25/04/08 14:43:00 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@5d8c704e[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@503f1cf1[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@6ec02593]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@c3a431e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "Exception in thread \"stream execution thread for [id = 753d6490-35b3-4e03-acd7-ecec8daaaa88, runId = 2f3bcea9-df73-436c-a8d9-5621021993ce]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:426)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:360)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:340)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:100)\n",
      "\t... 11 more\n",
      "25/04/08 14:43:00 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@2d2a22ff[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@a6a79fa[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@27d28ada]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@31004e2d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "Exception in thread \"stream execution thread for [id = a260ec59-37a1-4ada-bf0e-b3ae2ba797e9, runId = a8e7f450-6607-4300-9a01-a218be27302d]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:426)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:360)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:340)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:100)\n",
      "\t... 11 more\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |word   |count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2025-04-08 14:42:40, 2025-04-08 14:43:00}|datos  |1    |\n",
      "|{2025-04-08 14:42:30, 2025-04-08 14:42:50}|canse  |1    |\n",
      "|{2025-04-08 14:42:20, 2025-04-08 14:42:40}|holi   |1    |\n",
      "|{2025-04-08 14:42:30, 2025-04-08 14:42:50}|holi   |1    |\n",
      "|{2025-04-08 14:42:40, 2025-04-08 14:43:00}|benja  |1    |\n",
      "|{2025-04-08 14:42:50, 2025-04-08 14:43:10}|Luis   |1    |\n",
      "|{2025-04-08 14:42:30, 2025-04-08 14:42:50}|benja  |1    |\n",
      "|{2025-04-08 14:42:40, 2025-04-08 14:43:00}|       |1    |\n",
      "|{2025-04-08 14:42:30, 2025-04-08 14:42:50}|candada|1    |\n",
      "|{2025-04-08 14:42:40, 2025-04-08 14:43:00}|grandes|1    |\n",
      "|{2025-04-08 14:42:40, 2025-04-08 14:43:00}|Luis   |1    |\n",
      "|{2025-04-08 14:42:20, 2025-04-08 14:42:40}|candada|1    |\n",
      "|{2025-04-08 14:42:30, 2025-04-08 14:42:50}|bigD   |1    |\n",
      "|{2025-04-08 14:42:30, 2025-04-08 14:42:50}|me     |1    |\n",
      "|{2025-04-08 14:42:20, 2025-04-08 14:42:40}|canse  |1    |\n",
      "|{2025-04-08 14:42:40, 2025-04-08 14:43:00}|bigD   |1    |\n",
      "|{2025-04-08 14:42:20, 2025-04-08 14:42:40}|me     |1    |\n",
      "|{2025-04-08 14:42:50, 2025-04-08 14:43:10}|grandes|1    |\n",
      "|{2025-04-08 14:42:30, 2025-04-08 14:42:50}|ya     |1    |\n",
      "|{2025-04-08 14:42:20, 2025-04-08 14:42:40}|ya     |1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |word   |count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2025-04-08 14:42:30, 2025-04-08 14:42:50}|canse  |1    |\n",
      "|{2025-04-08 14:43:00, 2025-04-08 14:43:20}|Luis   |1    |\n",
      "|{2025-04-08 14:42:40, 2025-04-08 14:43:00}|benja  |1    |\n",
      "|{2025-04-08 14:42:50, 2025-04-08 14:43:10}|Luis   |1    |\n",
      "|{2025-04-08 14:42:40, 2025-04-08 14:43:00}|datos  |1    |\n",
      "|{2025-04-08 14:42:20, 2025-04-08 14:42:40}|holi   |1    |\n",
      "|{2025-04-08 14:42:30, 2025-04-08 14:42:50}|holi   |1    |\n",
      "|{2025-04-08 14:42:30, 2025-04-08 14:42:50}|benja  |1    |\n",
      "|{2025-04-08 14:42:40, 2025-04-08 14:43:00}|       |1    |\n",
      "|{2025-04-08 14:43:10, 2025-04-08 14:43:30}|�Luis  |1    |\n",
      "|{2025-04-08 14:42:20, 2025-04-08 14:42:40}|candada|1    |\n",
      "|{2025-04-08 14:43:10, 2025-04-08 14:43:30}|Sam    |2    |\n",
      "|{2025-04-08 14:43:00, 2025-04-08 14:43:20}|Sam    |2    |\n",
      "|{2025-04-08 14:42:30, 2025-04-08 14:42:50}|candada|1    |\n",
      "|{2025-04-08 14:42:40, 2025-04-08 14:43:00}|grandes|1    |\n",
      "|{2025-04-08 14:43:10, 2025-04-08 14:43:30}|Luis   |1    |\n",
      "|{2025-04-08 14:43:00, 2025-04-08 14:43:20}|que    |1    |\n",
      "|{2025-04-08 14:42:40, 2025-04-08 14:43:00}|Luis   |1    |\n",
      "|{2025-04-08 14:42:30, 2025-04-08 14:42:50}|me     |1    |\n",
      "|{2025-04-08 14:42:20, 2025-04-08 14:42:40}|canse  |1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "query = windowed_counts \\\n",
    "                .writeStream \\\n",
    "                .outputMode(\"complete\") \\\n",
    "                .trigger(processingTime='30 seconds') \\\n",
    "                .format(\"console\") \\\n",
    "                .option(\"truncate\", \"false\") \\\n",
    "                .start()\n",
    "\n",
    "query.awaitTermination(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "words = words.withColumn(\"count\", lit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "windowed_sum =  words \\\n",
    "                        .withWatermark(\"timestamp\", \"2 minutes\") \\\n",
    "                        .groupBy(window(words.timestamp, \n",
    "                                        \"30 seconds\", # Window duration \n",
    "                                        \"10 seconds\"), # Slide duration\n",
    "                                 words.word) \\\n",
    "                        .agg(sum(\"count\").alias(\"Sum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 14:44:07 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9c966d9b-fd11-4ed8-a7bf-3ccdd2df307c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/04/08 14:44:07 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 14:44:07 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----+---+\n",
      "|window|word|Sum|\n",
      "+------+----+---+\n",
      "+------+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-----+---+\n",
      "|window                                    |word |Sum|\n",
      "+------------------------------------------+-----+---+\n",
      "|{2025-04-08 14:43:50, 2025-04-08 14:44:20}|Luis |2  |\n",
      "|{2025-04-08 14:44:20, 2025-04-08 14:44:50}|Luis |2  |\n",
      "|{2025-04-08 14:44:20, 2025-04-08 14:44:50}|Benja|2  |\n",
      "|{2025-04-08 14:44:10, 2025-04-08 14:44:40}|Sam  |2  |\n",
      "|{2025-04-08 14:44:10, 2025-04-08 14:44:40}|Gatos|2  |\n",
      "|{2025-04-08 14:44:00, 2025-04-08 14:44:30}|Sam  |2  |\n",
      "|{2025-04-08 14:43:50, 2025-04-08 14:44:20}|Sam  |2  |\n",
      "|{2025-04-08 14:44:00, 2025-04-08 14:44:30}|Benja|4  |\n",
      "|{2025-04-08 14:44:10, 2025-04-08 14:44:40}|Luis |4  |\n",
      "|{2025-04-08 14:44:10, 2025-04-08 14:44:40}|Benja|4  |\n",
      "|{2025-04-08 14:44:00, 2025-04-08 14:44:30}|Gatos|2  |\n",
      "|{2025-04-08 14:44:20, 2025-04-08 14:44:50}|Gatos|2  |\n",
      "|{2025-04-08 14:43:50, 2025-04-08 14:44:20}|Benja|2  |\n",
      "|{2025-04-08 14:44:00, 2025-04-08 14:44:30}|Luis |4  |\n",
      "+------------------------------------------+-----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+---+\n",
      "|window                                    |word   |Sum|\n",
      "+------------------------------------------+-------+---+\n",
      "|{2025-04-08 14:43:50, 2025-04-08 14:44:20}|Luis   |2  |\n",
      "|{2025-04-08 14:44:20, 2025-04-08 14:44:50}|Luis   |4  |\n",
      "|{2025-04-08 14:44:20, 2025-04-08 14:44:50}|Benja  |2  |\n",
      "|{2025-04-08 14:44:10, 2025-04-08 14:44:40}|Gatos  |2  |\n",
      "|{2025-04-08 14:44:30, 2025-04-08 14:45:00}|BIGDATA|4  |\n",
      "|{2025-04-08 14:44:10, 2025-04-08 14:44:40}|BIGDATA|4  |\n",
      "|{2025-04-08 14:44:10, 2025-04-08 14:44:40}|Sam    |2  |\n",
      "|{2025-04-08 14:44:00, 2025-04-08 14:44:30}|Sam    |2  |\n",
      "|{2025-04-08 14:43:50, 2025-04-08 14:44:20}|Sam    |2  |\n",
      "|{2025-04-08 14:44:00, 2025-04-08 14:44:30}|Benja  |4  |\n",
      "|{2025-04-08 14:44:10, 2025-04-08 14:44:40}|Luis   |6  |\n",
      "|{2025-04-08 14:44:20, 2025-04-08 14:44:50}|BIGDATA|4  |\n",
      "|{2025-04-08 14:44:10, 2025-04-08 14:44:40}|Benja  |4  |\n",
      "|{2025-04-08 14:44:00, 2025-04-08 14:44:30}|Gatos  |2  |\n",
      "|{2025-04-08 14:44:30, 2025-04-08 14:45:00}|Luis   |2  |\n",
      "|{2025-04-08 14:44:00, 2025-04-08 14:44:30}|Luis   |4  |\n",
      "|{2025-04-08 14:44:20, 2025-04-08 14:44:50}|Gatos  |2  |\n",
      "|{2025-04-08 14:43:50, 2025-04-08 14:44:20}|Benja  |2  |\n",
      "+------------------------------------------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "query2 = windowed_sum \\\n",
    "                .writeStream \\\n",
    "                .outputMode(\"complete\") \\\n",
    "                .trigger(processingTime='30 seconds') \\\n",
    "                .format(\"console\") \\\n",
    "                .option(\"truncate\", \"false\") \\\n",
    "                .start()\n",
    "\n",
    "query2.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "words = words.withColumn(\"len\", length(words.word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "windowed_sum =  words \\\n",
    "                        .withWatermark(\"timestamp\", \"2 minutes\") \\\n",
    "                        .groupBy(window(words.timestamp, \n",
    "                                        \"30 seconds\", # Window duration \n",
    "                                        \"10 seconds\"), # Slide duration\n",
    "                                 words.word) \\\n",
    "                        .agg(avg(\"len\").alias(\"Avg len\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 14:47:19 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4138abb1-b4c7-4852-9de9-d57023e7de07. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/04/08 14:47:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/04/08 14:47:19 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----+-------+\n",
      "|window|word|Avg len|\n",
      "+------+----+-------+\n",
      "+------+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-----+---+\n",
      "|window                                    |word |Sum|\n",
      "+------------------------------------------+-----+---+\n",
      "|{2025-04-08 14:45:40, 2025-04-08 14:46:10}|Hola |2  |\n",
      "|{2025-04-08 14:45:50, 2025-04-08 14:46:20}|datos|2  |\n",
      "|{2025-04-08 14:45:30, 2025-04-08 14:46:00}|datos|2  |\n",
      "|{2025-04-08 14:47:00, 2025-04-08 14:47:30}|Len  |2  |\n",
      "|{2025-04-08 14:45:40, 2025-04-08 14:46:10}|avg  |2  |\n",
      "|{2025-04-08 14:46:00, 2025-04-08 14:46:30}|perro|2  |\n",
      "|{2025-04-08 14:45:40, 2025-04-08 14:46:10}|sum  |2  |\n",
      "|{2025-04-08 14:45:50, 2025-04-08 14:46:20}|sum  |2  |\n",
      "|{2025-04-08 14:45:50, 2025-04-08 14:46:20}|adios|2  |\n",
      "|{2025-04-08 14:45:40, 2025-04-08 14:46:10}|datos|2  |\n",
      "|{2025-04-08 14:45:30, 2025-04-08 14:46:00}|Hola |2  |\n",
      "|{2025-04-08 14:45:50, 2025-04-08 14:46:20}|avg  |2  |\n",
      "|{2025-04-08 14:46:00, 2025-04-08 14:46:30}|avg  |2  |\n",
      "|{2025-04-08 14:45:40, 2025-04-08 14:46:10}|perro|2  |\n",
      "|{2025-04-08 14:47:20, 2025-04-08 14:47:50}|Sam  |4  |\n",
      "|{2025-04-08 14:45:30, 2025-04-08 14:46:00}|adios|2  |\n",
      "|{2025-04-08 14:47:10, 2025-04-08 14:47:40}|Sam  |4  |\n",
      "|{2025-04-08 14:47:10, 2025-04-08 14:47:40}|Len  |2  |\n",
      "|{2025-04-08 14:45:40, 2025-04-08 14:46:10}|adios|2  |\n",
      "|{2025-04-08 14:46:00, 2025-04-08 14:46:30}|sum  |2  |\n",
      "+------------------------------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----+-------+\n",
      "|window                                    |word|Avg len|\n",
      "+------------------------------------------+----+-------+\n",
      "|{2025-04-08 14:47:00, 2025-04-08 14:47:30}|Len |3.0    |\n",
      "|{2025-04-08 14:47:20, 2025-04-08 14:47:50}|Sam |3.0    |\n",
      "|{2025-04-08 14:47:10, 2025-04-08 14:47:40}|Len |3.0    |\n",
      "|{2025-04-08 14:47:10, 2025-04-08 14:47:40}|Sam |3.0    |\n",
      "|{2025-04-08 14:47:20, 2025-04-08 14:47:50}|Len |3.0    |\n",
      "|{2025-04-08 14:47:00, 2025-04-08 14:47:30}|Sam |3.0    |\n",
      "+------------------------------------------+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------+---+\n",
      "|window                                    |word     |Sum|\n",
      "+------------------------------------------+---------+---+\n",
      "|{2025-04-08 14:45:40, 2025-04-08 14:46:10}|Hola     |2  |\n",
      "|{2025-04-08 14:47:10, 2025-04-08 14:47:40}|promedio |2  |\n",
      "|{2025-04-08 14:45:50, 2025-04-08 14:46:20}|datos    |2  |\n",
      "|{2025-04-08 14:45:30, 2025-04-08 14:46:00}|datos    |2  |\n",
      "|{2025-04-08 14:47:00, 2025-04-08 14:47:30}|Len      |2  |\n",
      "|{2025-04-08 14:47:30, 2025-04-08 14:48:00}|promedio |2  |\n",
      "|{2025-04-08 14:45:40, 2025-04-08 14:46:10}|avg      |2  |\n",
      "|{2025-04-08 14:46:00, 2025-04-08 14:46:30}|perro    |2  |\n",
      "|{2025-04-08 14:47:30, 2025-04-08 14:48:00}|queaplica|2  |\n",
      "|{2025-04-08 14:45:40, 2025-04-08 14:46:10}|sum      |2  |\n",
      "|{2025-04-08 14:45:50, 2025-04-08 14:46:20}|sum      |2  |\n",
      "|{2025-04-08 14:45:50, 2025-04-08 14:46:20}|adios    |2  |\n",
      "|{2025-04-08 14:45:40, 2025-04-08 14:46:10}|datos    |2  |\n",
      "|{2025-04-08 14:47:40, 2025-04-08 14:48:10}|queaplica|2  |\n",
      "|{2025-04-08 14:45:30, 2025-04-08 14:46:00}|Hola     |2  |\n",
      "|{2025-04-08 14:45:50, 2025-04-08 14:46:20}|avg      |2  |\n",
      "|{2025-04-08 14:46:00, 2025-04-08 14:46:30}|avg      |2  |\n",
      "|{2025-04-08 14:45:40, 2025-04-08 14:46:10}|perro    |2  |\n",
      "|{2025-04-08 14:47:20, 2025-04-08 14:47:50}|Sam      |4  |\n",
      "|{2025-04-08 14:47:20, 2025-04-08 14:47:50}|queaplica|2  |\n",
      "+------------------------------------------+---------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------+-------+\n",
      "|window                                    |word     |Avg len|\n",
      "+------------------------------------------+---------+-------+\n",
      "|{2025-04-08 14:47:10, 2025-04-08 14:47:40}|promedio |8.0    |\n",
      "|{2025-04-08 14:47:00, 2025-04-08 14:47:30}|Len      |3.0    |\n",
      "|{2025-04-08 14:47:30, 2025-04-08 14:48:00}|promedio |8.0    |\n",
      "|{2025-04-08 14:47:30, 2025-04-08 14:48:00}|queaplica|9.0    |\n",
      "|{2025-04-08 14:47:40, 2025-04-08 14:48:10}|queaplica|9.0    |\n",
      "|{2025-04-08 14:47:20, 2025-04-08 14:47:50}|Sam      |3.0    |\n",
      "|{2025-04-08 14:47:10, 2025-04-08 14:47:40}|Len      |3.0    |\n",
      "|{2025-04-08 14:47:20, 2025-04-08 14:47:50}|promedio |8.0    |\n",
      "|{2025-04-08 14:47:20, 2025-04-08 14:47:50}|queaplica|9.0    |\n",
      "|{2025-04-08 14:47:10, 2025-04-08 14:47:40}|Sam      |3.0    |\n",
      "|{2025-04-08 14:47:00, 2025-04-08 14:47:30}|Sam      |3.0    |\n",
      "|{2025-04-08 14:47:20, 2025-04-08 14:47:50}|Len      |3.0    |\n",
      "+------------------------------------------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "query3 = windowed_sum \\\n",
    "                .writeStream \\\n",
    "                .outputMode(\"complete\") \\\n",
    "                .trigger(processingTime='30 seconds') \\\n",
    "                .format(\"console\") \\\n",
    "                .option(\"truncate\", \"false\") \\\n",
    "                .start()\n",
    "\n",
    "query3.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "query3.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
