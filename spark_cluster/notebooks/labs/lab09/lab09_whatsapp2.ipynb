{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **Ejemplos de Spark: Structured Streaming (Kafka + Watermarking)** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez\n",
    "\n",
    "**LAB**: LAB09\n",
    "\n",
    "**Estudiantes**: Angel Ramirez, Roberto Osorno, Yochabel Cazares, Samuel Romero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de la conexión con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLStructuredStreaming-Kafka-Watermarking\") \\\n",
    "    .master(\"spark://56a250e0d184:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_lines = spark \\\n",
    "                .readStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"5baf6a7c8147:9093\") \\\n",
    "                .option(\"subscribe\", \"kafka-spark-example\") \\\n",
    "                .load()\n",
    "\n",
    "kafka_lines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform binary data into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = kafka_lines.withColumn(\"value_str\", kafka_lines.value.cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = false)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "words = kafka_df.select(explode(split(kafka_df.value, \" \")).alias(\"word\"), \"timestamp\")\n",
    "words.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando el mecanismo para manejar datos tardios con marcas de agua (watermarking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "from pyspark.sql.functions import col, lit, window, count, avg, min, max\n",
    "\n",
    "\n",
    "word_counts = words \\\n",
    "    .withWatermark(\"timestamp\", \"2 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"60 seconds\", \"30 seconds\"),\n",
    "        col(\"word\")\n",
    "    ) \\\n",
    "    .agg(count(\"*\").alias(\"word_count\"))\n",
    "\n",
    "\n",
    "window_stats = word_counts \\\n",
    "    .groupBy(\"window\") \\\n",
    "    .agg(\n",
    "        avg(\"word_count\").alias(\"avg_count\"),\n",
    "        min(\"word_count\").alias(\"min_count\"),\n",
    "        max(\"word_count\").alias(\"max_count\")\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del \"Sink\" del stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 14:41:09 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4f7055fd-e0b2-48d5-96e1-99700a8d66d3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/04/08 14:41:09 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/04/08 14:41:09 WARN UnsupportedOperationChecker: Detected pattern of possible 'correctness' issue due to global watermark. The query contains stateful operation which can emit rows older than the current watermark plus allowed late record delay, which are \"late rows\" in downstream stateful operations and these rows can be discarded. Please refer the programming guide doc for more details. If you understand the possible risk of correctness issue and still need to run the query, you can disable this check by setting the config `spark.sql.streaming.statefulOperator.checkCorrectness.enabled` to false.;\n",
      "Aggregate [window#6443-T120000ms], [window#6443-T120000ms, avg(word_count#6447L) AS avg_count#6456, min(word_count#6447L) AS min_count#6458L, max(word_count#6447L) AS max_count#6460L]\n",
      "+- Aggregate [window#6448-T120000ms, word#6440], [window#6448-T120000ms AS window#6443-T120000ms, word#6440, count(1) AS word_count#6447L]\n",
      "   +- Filter isnotnull(timestamp#6421-T120000ms)\n",
      "      +- Expand [[named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - 0) % 30000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - 0) % 30000000) + 30000000) ELSE ((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - 0) % 30000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - 0) % 30000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - 0) % 30000000) + 30000000) ELSE ((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - 0) % 30000000) END) - 0) + 60000000), LongType, TimestampType))), word#6440, timestamp#6421-T120000ms], [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - 0) % 30000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - 0) % 30000000) + 30000000) ELSE ((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - 0) % 30000000) END) - 30000000), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - 0) % 30000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - 0) % 30000000) + 30000000) ELSE ((precisetimestampconversion(timestamp#6421-T120000ms, TimestampType, LongType) - 0) % 30000000) END) - 30000000) + 60000000), LongType, TimestampType))), word#6440, timestamp#6421-T120000ms]], [window#6448-T120000ms, word#6440, timestamp#6421-T120000ms]\n",
      "         +- EventTimeWatermark timestamp#6421: timestamp, 2 minutes\n",
      "            +- Project [word#6440, timestamp#6421]\n",
      "               +- Generate explode(split(cast(value#6417 as string),  , -1)), false, [word#6440]\n",
      "                  +- Project [key#6416, value#6417, topic#6418, partition#6419, offset#6420L, timestamp#6421, timestampType#6422, cast(value#6417 as string) AS value_str#6430]\n",
      "                     +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@30bd8d5a, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@57a75263, [kafka.bootstrap.servers=5baf6a7c8147:9093, subscribe=kafka-spark-example], [key#6416, value#6417, topic#6418, partition#6419, offset#6420L, timestamp#6421, timestampType#6422], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@547d4167,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> 5baf6a7c8147:9093, subscribe -> kafka-spark-example),None), kafka, [key#6409, value#6410, topic#6411, partition#6412, offset#6413L, timestamp#6414, timestampType#6415]\n",
      "\n",
      "25/04/08 14:41:09 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+---------+---------+---------+\n",
      "|window|avg_count|min_count|max_count|\n",
      "+------+---------+---------+---------+\n",
      "+------+---------+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 14:41:30 ERROR MicroBatchExecution: Query [id = 524f3f08-452e-4fe6-a7ae-621965fb6be7, runId = c4ecf413-f9f4-4f00-a17d-390227d8a8c1] terminated with error\n",
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\t... 11 more\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-----------------+---------+---------+\n",
      "|window                                    |avg_count        |min_count|max_count|\n",
      "+------------------------------------------+-----------------+---------+---------+\n",
      "|{2025-04-08 14:41:00, 2025-04-08 14:42:00}|6.333333333333333|1        |15       |\n",
      "|{2025-04-08 14:40:30, 2025-04-08 14:41:30}|6.333333333333333|1        |15       |\n",
      "+------------------------------------------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Disable the correctness check for stateful operations\n",
    "spark.conf.set(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "query = window_stats \\\n",
    "                .writeStream \\\n",
    "                .outputMode(\"update\") \\\n",
    "                .trigger(processingTime='30 seconds') \\\n",
    "                .format(\"console\") \\\n",
    "                .option(\"truncate\", \"false\") \\\n",
    "                .start()\n",
    "\n",
    "query.awaitTermination(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
